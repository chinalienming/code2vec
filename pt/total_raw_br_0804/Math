1	src/main/java/org/apache/commons/math3/fraction/Fraction.java内methodPartNotInFile:private Fraction(double value, double epsilon, int maxDenominator, int maxIterat外src/main/java/org/apache/commons/math3/fraction/BigFraction.java内private BigFraction(final double value, final double epsilon, final int maxDenominator, int maxIterations)	1780da711d88f669e29e0517f83d692fb80099db	0da657a65c92b086a301a6ffe9e34ec272f8889c	MATH-996	Fraction specified with maxDenominator and a value very close to a simple fraction should not throw an overflow exception	An overflow exception is thrown when a Fraction is initialized with a maxDenominator from a double that is very close to a simple fraction.  For example: double d = 0.5000000001; Fraction f = new Fraction(d, 10); Patch with unit test on way.	https://issues.apache.org/jira/browse/MATH-996	src/main/java
2	src/main/java/org/apache/commons/math3/distribution/HypergeometricDistribution.java内private double innerCumulativeProbability(int x0, int x1, int dx)	ad5c90bbabdbce834068bc79d3eb00c823e97008	c0b655ace5665c0cd32e3f5e5b46edad4d223125	MATH-1021	HypergeometricDistribution.sample suffers from integer overflow	Hi, I have an application which broke when ported from commons math 2.2 to 3.2. It looks like the HypergeometricDistribution.sample() method doesn't work as well as it used to with large integer values – the example code below should return a sample between 0 and 50, but usually returns -50.  import org.apache.commons.math3.distribution.HypergeometricDistribution;  public class Foo {   public static void main(String[] args) {     HypergeometricDistribution a = new HypergeometricDistribution(         43130568, 42976365, 50);     System.out.printf("%d %d%n", a.getSupportLowerBound(), a.getSupportUpperBound()); // Prints "0 50"     System.out.printf("%d%n",a.sample());                                             // Prints "-50"   } }   In the debugger, I traced it as far as an integer overflow in HypergeometricDistribution.getNumericalMean() – instead of doing  return (double) (getSampleSize() * getNumberOfSuccesses()) / (double) getPopulationSize();   it could do:  return getSampleSize() * ((double) getNumberOfSuccesses() / (double) getPopulationSize());   This seemed to fix it, based on a quick test.	https://issues.apache.org/jira/browse/MATH-1021	src/main/java
3	src/main/java/org/apache/commons/math3/util/MathArrays.java内public static double linearCombination(final double[] a, final double[] b)	de4209544270def43e39db0d214d1564939f8e40	92c4697300e8ddf06152bc0838c738d50673c1d2	MATH-1005	ArrayIndexOutOfBoundsException in MathArrays.linearCombination	When MathArrays.linearCombination is passed arguments with length 1, it throws an ArrayOutOfBoundsException. This is caused by this line: double prodHighNext = prodHigh[1]; linearCombination should check the length of the arguments and fall back to simple multiplication if length == 1.	https://issues.apache.org/jira/browse/MATH-1005	src/main/java
4	src/main/java/org/apache/commons/math3/geometry/euclidean/twod/SubLine.java内methodPartNotInFile:public Vector2D intersection(final SubLine subLine, final boolean includeEndPoin外src/main/java/org/apache/commons/math3/geometry/euclidean/threed/SubLine.java内methodPartNotInFile:public Vector3D intersection(final SubLine subLine, final boolean includeEndPoin	d609e65f7bfb38c66e2a8670242e72108b3d0e98	277e61721f34be16a20da663fd597edf6b51939b	MATH-988	NPE when calling SubLine.intersection() with non-intersecting lines	When calling SubLine.intersection() with two lines that not intersect, then a NullPointerException is thrown in Line.toSubSpace(). This bug is in the twod and threed implementations. The attached patch fixes both implementations and adds the required test cases. 	https://issues.apache.org/jira/browse/MATH-988	src/main/java
5	src/main/java/org/apache/commons/math3/complex/Complex.java内public Complex reciprocal()	f5bcba812fb8bc59ff5e4bc055811039b610aa2b	e54a1c92302f3167b06bc04859b87ac0681bcdf3	MATH-934	Complex.ZERO.reciprocal() returns NaN but should return INF.	Complex.ZERO.reciprocal() returns NaN but should return INF. Class: org.apache.commons.math3.complex.Complex; Method: reciprocal() @version $Id: Complex.java 1416643 2012-12-03 19:37:14Z tn $	https://issues.apache.org/jira/browse/MATH-934	src/main/java
6	src/main/java/org/apache/commons/math3/optim/BaseOptimizer.java内protected BaseOptimizer(ConvergenceChecker<PAIR> checker)外src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java内protected PointVectorValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/LevenbergMarquardtOptimizer.java内protected PointVectorValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java内public int compare(final PointValuePair o1, final PointValuePair o2)外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/SimplexOptimizer.java内public int compare(final PointValuePair o1, final PointValuePair o2)外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java内protected PointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/PowellOptimizer.java内protected PointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/noderiv/CMAESOptimizer.java内protected PointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java内protected PointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/scalar/gradient/NonLinearConjugateGradientOptimizer.java内protected PointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java内public PointVectorValuePair doOptimize()外src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/GaussNewtonOptimizer.java内public PointVectorValuePair doOptimize()	6c9d06a658d87c01fb02d1efce15bf6b74eb7aab	419a052c6842192e78f747d9f5af619c2ca56e78	MATH-949	LevenbergMarquardtOptimizer reports 0 iterations	The method LevenbergMarquardtOptimizer.getIterations() does not report the correct number of iterations; It always returns 0. A quick look at the code shows that only SimplexOptimizer calls BaseOptimizer.incrementEvaluationsCount() I've put a test case below. Notice how the evaluations count is correctly incremented, but the iterations count is not.      @Test     public void testGetIterations() {         // setup         LevenbergMarquardtOptimizer otim = new LevenbergMarquardtOptimizer();          // action         otim.optimize(new MaxEval(100), new Target(new double[] { 1 }),                 new Weight(new double[] { 1 }), new InitialGuess(                         new double[] { 3 }), new ModelFunction(                         new MultivariateVectorFunction() {                             @Override                             public double[] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[] { FastMath.pow(point[0], 4) };                             }                         }), new ModelFunctionJacobian(                         new MultivariateMatrixFunction() {                             @Override                             public double[][] value(double[] point)                                     throws IllegalArgumentException {                                 return new double[][] { { 0.25 * FastMath.pow(                                         point[0], 3) } };                             }                         }));          // verify         assertThat(otim.getEvaluations(), greaterThan(1));         assertThat(otim.getIterations(), greaterThan(1));     }   	https://issues.apache.org/jira/browse/MATH-949	src/main/java
7	src/main/java/org/apache/commons/math3/ode/AbstractIntegrator.java内public int compare(EventState es0, EventState es1)外src/main/java/org/apache/commons/math3/ode/AbstractIntegrator.java内public int compare(EventState es0, EventState es1)	96cd02e82c219349086092cdc2c5a450efc8be13	1cd68eed57febf806c385bf04a596b922f4c1964	MATH-950	event state not updated if an unrelated event triggers a RESET_STATE during ODE integration	When an ODE solver manages several different event types, there are some unwanted side effects. If one event handler asks for a RESET_STATE (for integration state) when its eventOccurred method is called, the other event handlers that did not trigger an event in the same step are not updated correctly, due to an early return. As a result, when the next step is processed with a reset integration state, the forgotten event still refer to the start date of the previous state. This implies that when these event handlers will be checked for In some cases, the function defining an event g(double t, double[] y) is called with state parameters y that are completely wrong. In one case when the y array should have contained values between -1 and +1, one function call got values up to 1.0e20. The attached file reproduces the problem.	https://issues.apache.org/jira/browse/MATH-950	src/main/java
8	src/main/java/org/apache/commons/math3/distribution/DiscreteDistribution.java内public T sample()	55046479fe33e47a002ab4f4949bd944ba70d37d	196e63174afdb3df1529c1b97bb8437b16831cc3	MATH-942	DiscreteDistribution.sample(int) may throw an exception if first element of singletons of sub-class type	Creating an array with Array.newInstance(singletons.get(0).getClass(), sampleSize) in DiscreteDistribution.sample(int) is risky. An exception will be thrown if:  singleons.get(0) is of type T1, an sub-class of T, and DiscreteDistribution.sample() returns an object which is of type T, but not of type T1.  To reproduce:  List<Pair<Object,Double>> list = new ArrayList<Pair<Object, Double>>(); list.add(new Pair<Object, Double>(new Object() {}, new Double(0))); list.add(new Pair<Object, Double>(new Object() {}, new Double(1))); new DiscreteDistribution<Object>(list).sample(1);   Attaching a patch.	https://issues.apache.org/jira/browse/MATH-942	src/main/java
9	src/main/java/org/apache/commons/math3/geometry/euclidean/threed/Line.java内methodPartNotInFile:public void reset(final Vector3D p1, final Vector3D p2) throws MathIllegalArgume	c929aee12a2b777152371fc061e4b79f088fa039	de98c0f0a566060ac143c39036f06a03f141dc52	MATH-938	Line.revert() is imprecise	Line.revert() only maintains ~10 digits for the direction. This becomes an issue when the line's position is evaluated far from the origin. A simple fix would be to use Vector3D.negate() for the direction. Also, is there a reason why Line is not immutable? It is just comprised of two vectors.	https://issues.apache.org/jira/browse/MATH-938	src/main/java
10	src/main/java/org/apache/commons/math3/analysis/differentiation/DSCompiler.java内public void atan2(final double[] y, final int yOffset, final double[] x, final int xOffset, final double[] result, final int resultOffset)	d2a916e325a1c47f12137ea902aaac100ed1f5db	7e2ffcc9034de41d7787f0b33b5670474f7a10de	MATH-935	DerivativeStructure.atan2(y,x) does not handle special cases properly	The four special cases +/-0 for both x and y should give the same values as Math.atan2 and FastMath.atan2. However, they give NaN for the value in all cases.	https://issues.apache.org/jira/browse/MATH-935	src/main/java
11	src/main/java/org/apache/commons/math3/distribution/MultivariateNormalDistribution.java内public double density(final double[] vals)	02f64e7355618ef222c2320bd6731cce8c244fc5	aa2bd1d0656b0001192aa2b2ef779cfd1f3b7e4d	MATH-929	MultivariateNormalDistribution.density(double[]) returns wrong value when the dimension is odd	To reproduce:  Assert.assertEquals(0.398942280401433, new MultivariateNormalDistribution(new double[]{0}, new double[][]{{1}}).density(new double[]{0}), 1e-15);  	https://issues.apache.org/jira/browse/MATH-929	src/main/java
12	src/main/java/org/apache/commons/math3/random/BitsStreamGenerator.java内noLeftCurtyIn1stLine:外src/main/java/org/apache/commons/math3/random/BitsStreamGenerator.java内noLeftCurtyIn1stLine:	67fc870987c8da66ae719764a6ce2e1a9d3bfc76	cc82d38aaf6ec24758a0a6df9535b8204e22ef12	MATH-927	GammaDistribution cloning broken	Serializing a GammaDistribution and deserializing it, does not result in a cloned distribution that produces the same samples. Cause: GammaDistribution inherits from AbstractRealDistribution, which implements Serializable. AbstractRealDistribution has random, in which we have a Well19937c instance, which inherits from AbstractWell. AbstractWell implements Serializable. AbstractWell inherits from BitsStreamGenerator, which is not Serializable, but does have a private field 'nextGaussian'. Solution: Make BitStreamGenerator implement Serializable as well. This probably affects other distributions as well.	https://issues.apache.org/jira/browse/MATH-927	src/main/java
13	src/main/java/org/apache/commons/math3/optimization/general/AbstractLeastSquaresOptimizer.java内protected void setUp()	a6b2e992e17cee0d4cb5a2da8242a20b8e5a8fc3	8079ea5b8d1366445da532906e43afa9291473cf	MATH-924	new multivariate vector optimizers cannot be used with large number of weights	When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large. This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.	https://issues.apache.org/jira/browse/MATH-924	src/main/java
14	src/main/java/org/apache/commons/math3/optim/nonlinear/vector/jacobian/AbstractLeastSquaresOptimizer.java内private void parseOptimizationData(OptimizationData... optData)外src/main/java/org/apache/commons/math3/optim/nonlinear/vector/Weight.java内noLeftCurtyIn1stLine:	0f37f87216b67d12b722d706f3e313b731ee31ab	a6b2e992e17cee0d4cb5a2da8242a20b8e5a8fc3	MATH-924	new multivariate vector optimizers cannot be used with large number of weights	When using the Weigth class to pass a large number of weights to multivariate vector optimizers, an nxn full matrix is created (and copied) when a n elements vector is used. This exhausts memory when n is large. This happens for example when using curve fitters (even simple curve fitters like polynomial ones for low degree) with large number of points. I encountered this with curve fitting on 41200 points, which created a matrix with 1.7 billion elements.	https://issues.apache.org/jira/browse/MATH-924	src/main/java
15	src/main/java/org/apache/commons/math3/util/FastMath.java内noLeftCurtyIn1stLine:外src/main/java/org/apache/commons/math3/util/FastMath.java内public static double pow(double x, double y)	efcf91bce70116ab1f628dee5c9ed1c2f18d2db9	b221a7342856a2a548f5d9817055defc0ec4dea6	MATH-904	FastMath.pow deviates from Math.pow for negative, finite base values with an exponent 2^52 < y < 2^53	As reported by Jeff Hain: pow(double,double): Math.pow(-1.0,5.000000000000001E15) = -1.0 FastMath.pow(-1.0,5.000000000000001E15) = 1.0 ===> This is due to considering that power is an even integer if it is >= 2^52, while you need to test that it is >= 2^53 for it. ===> replace "if (y >= TWO_POWER_52 || y <= -TWO_POWER_52)" with "if (y >= 2*TWO_POWER_52 || y <= -2*TWO_POWER_52)" and that solves it.	https://issues.apache.org/jira/browse/MATH-904	src/main/java
16	src/main/java/org/apache/commons/math3/util/FastMath.java内noLeftCurtyIn1stLine:外src/main/java/org/apache/commons/math3/util/FastMath.java内public static double cosh(double x)外src/main/java/org/apache/commons/math3/util/FastMath.java内public static double sinh(double x)	87324e56a31c110a1653955cdd715e9e88a54ed7	875ed1f4d90d1457c0ab40dafc79be5a0c6f9bf6	MATH-905	FastMath.[cosh, sinh] do not support the same range of values as the Math counterparts	As reported by Jeff Hain: cosh(double) and sinh(double): Math.cosh(709.783) = 8.991046692770538E307 FastMath.cosh(709.783) = Infinity Math.sinh(709.783) = 8.991046692770538E307 FastMath.sinh(709.783) = Infinity ===> This is due to using exp( x )/2 for values of |x| above 20: the result sometimes should not overflow, but exp( x ) does, so we end up with some infinity. ===> for values of |x| >= StrictMath.log(Double.MAX_VALUE), exp will overflow, so you need to use that instead: for x positive: double t = exp(x*0.5); return (0.5*t)*t; for x negative: double t = exp(-x*0.5); return (-0.5*t)*t;	https://issues.apache.org/jira/browse/MATH-905	src/main/java
17	src/main/java/org/apache/commons/math3/dfp/Dfp.java内public Dfp multiply(final Dfp x)	07611165b6176b6e3e6d5ac6ca052a102f10e3c4	621806b796bc416f00341feca894ebae07be5ed0	MATH-778	Dfp Dfp.multiply(int x) does not comply with the general contract FieldElement.multiply(int n)	In class org.apache.commons.math3.Dfp,  the method multiply(int n) is limited to 0 <= n <= 9999. This is not consistent with the general contract of FieldElement.multiply(int n), where there should be no limitation on the values of n.	https://issues.apache.org/jira/browse/MATH-778	src/main/java
18	src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java内public FitnessFunction()外src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java内public FitnessFunction()外src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java内public boolean isFeasible(final double[] x)	6e62aefca7aefe26a2ebc43fcdcd6cd58953f7de	7c7d7e8f103582e753c39a2baf14a483e991fefb	MATH-867	CMAESOptimizer with bounds fits finely near lower bound and coarsely near upper bound.	When fitting with bounds, the CMAESOptimizer fits finely near the lower bound and coarsely near the upper bound.  This is because it internally maps the fitted parameter range into the interval [0,1].  The unit of least precision (ulp) between floating point numbers is much smaller near zero than near one.  Thus, fits have much better resolution near the lower bound (which is mapped to zero) than the upper bound (which is mapped to one).  I will attach a example program to demonstrate.	https://issues.apache.org/jira/browse/MATH-867	src/main/java
19	src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java内private void checkParameters()	efa9de05114492ca38cf4739a07339f5ad6faddc	c73fad0a0d42103b5e13a68317ea95b1090263ba	MATH-865	Wide bounds to CMAESOptimizer result in NaN parameters passed to fitness function	If you give large values as lower/upper bounds (for example -Double.MAX_VALUE as a lower bound), the optimizer can call the fitness function with parameters set to NaN.  My guess is this is due to FitnessFunction.encode/decode generating NaN when normalizing/denormalizing parameters.  For example, if the difference between the lower and upper bound is greater than Double.MAX_VALUE, encode could divide infinity by infinity.	https://issues.apache.org/jira/browse/MATH-865	src/main/java
20	src/main/java/org/apache/commons/math3/optimization/direct/CMAESOptimizer.java内public FitnessFunction()	5a50073d07ca309d78cfc4e071f4bbe051bdbd40	efa9de05114492ca38cf4739a07339f5ad6faddc	MATH-864	CMAESOptimizer does not enforce bounds	The CMAESOptimizer can exceed the bounds passed to optimize.  Looking at the generationLoop in doOptimize(), it does a bounds check by calling isFeasible() but if checkFeasableCount is zero (the default) then isFeasible() is never even called.  Also, even with non-zero checkFeasableCount it may give up before finding an in-bounds offspring and go forward with an out-of-bounds offspring.  This is against svn revision 1387637.  I can provide an example program where the optimizer ends up with a fit outside the prescribed bounds if that would help.	https://issues.apache.org/jira/browse/MATH-864	src/main/java
21	src/main/java/org/apache/commons/math3/linear/RectangularCholeskyDecomposition.java内public RectangularCholeskyDecomposition(RealMatrix matrix, double small)外src/main/java/org/apache/commons/math3/linear/RectangularCholeskyDecomposition.java内public RectangularCholeskyDecomposition(RealMatrix matrix, double small)外src/main/java/org/apache/commons/math3/linear/RectangularCholeskyDecomposition.java内public RectangularCholeskyDecomposition(RealMatrix matrix, double small)	dbb408e860279d92b7502474328bb5385c2d9243	468ed8550b7759858a4dc59d694859a8ae7d35f0	MATH-789	Correlated random vector generator fails (silently) when faced with zero rows in covariance matrix	The following three matrices (which are basically permutations of each other) produce different results when sampling a multi-variate Gaussian with the help of CorrelatedRandomVectorGenerator (sample covariances calculated in R, based on 10,000 samples): Array2DRowRealMatrix { {0.0,0.0,0.0,0.0,0.0} , {0.0,0.013445532,0.01039469,0.009881156,0.010499559} , {0.0,0.01039469,0.023006616,0.008196856,0.010732709} , {0.0,0.009881156,0.008196856,0.019023866,0.009210099} , {0.0,0.010499559,0.010732709,0.009210099,0.019107243}} > cov(data1)    V1 V2 V3 V4 V5 V1 0 0.000000000 0.00000000 0.000000000 0.000000000 V2 0 0.013383931 0.01034401 0.009913271 0.010506733 V3 0 0.010344006 0.02309479 0.008374730 0.010759306 V4 0 0.009913271 0.00837473 0.019005488 0.009187287 V5 0 0.010506733 0.01075931 0.009187287 0.019021483 Array2DRowRealMatrix { {0.013445532,0.01039469,0.0,0.009881156,0.010499559} , {0.01039469,0.023006616,0.0,0.008196856,0.010732709} , {0.0,0.0,0.0,0.0,0.0}, {0.009881156,0.008196856,0.0,0.019023866,0.009210099}, {0.010499559,0.010732709,0.0,0.009210099,0.019107243}}  > cov(data2)             V1 V2 V3 V4 V5 V1 0.006922905 0.010507692 0 0.005817399 0.010330529 V2 0.010507692 0.023428918 0 0.008273152 0.010735568 V3 0.000000000 0.000000000 0 0.000000000 0.000000000 V4 0.005817399 0.008273152 0 0.004929843 0.009048759 V5 0.010330529 0.010735568 0 0.009048759 0.018683544   Array2DRowRealMatrix{ {0.013445532,0.01039469,0.009881156,0.010499559}, {0.01039469,0.023006616,0.008196856,0.010732709}, {0.009881156,0.008196856,0.019023866,0.009210099}, {0.010499559,0.010732709,0.009210099,0.019107243}}  > cov(data3)             V1          V2          V3          V4 V1 0.013445047 0.010478862 0.009955904 0.010529542 V2 0.010478862 0.022910522 0.008610113 0.011046353 V3 0.009955904 0.008610113 0.019250975 0.009464442 V4 0.010529542 0.011046353 0.009464442 0.019260317   I've traced this back to the RectangularCholeskyDecomposition, which does not seem to handle the second matrix very well (decompositions in the same order as the matrices above):  CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0,0.0,0.0,0.0,0.0} , {0.0759577418122063,0.0876125188474239,0.0,0.0,0.0} , {0.07764443622513505,0.05132821221460752,0.11976381821791235,0.0,0.0} , {0.06662930527909404,0.05501661744114585,0.0016662506519307997,0.10749324207653632,0.0} ,{0.13822895138139477,0.0,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 5 CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.0}, {0.07764443622513505,0.13029949164628746,0.0} , {0.0,0.0,0.0} , {0.06662930527909404,0.023203936694855674,0.0} ,{0.13822895138139477,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 3 CorrelatedRandomVectorGenerator.getRootMatrix() =  Array2DRowRealMatrix{{0.0759577418122063,0.034512751379448724,0.033913748226348225,0.07303890149947785}, {0.07764443622513505,0.13029949164628746,0.0,0.0} , {0.06662930527909404,0.023203936694855674,0.11851573313229945,0.0} ,{0.13822895138139477,0.0,0.0,0.0}} CorrelatedRandomVectorGenerator.getRank() = 4 Clearly, the rank of each of these matrices should be 4. The first matrix does not lead to incorrect results, but the second one does. Unfortunately, I don't know enough about the Cholesky decomposition to find the flaw in the implementation, and I could not find documentation for the "rectangular" variant (also not at the links provided in the javadoc).	https://issues.apache.org/jira/browse/MATH-789	src/main/java
22	src/main/java/org/apache/commons/math3/distribution/UniformRealDistribution.java内public boolean isSupportLowerBoundInclusive()外src/main/java/org/apache/commons/math3/distribution/FDistribution.java内public double getSupportUpperBound()	51461f1c9ce91953ed3e6dbd01b0e2c8bbd75da2	b0cba9a79ee99b09958dec8e40c75bed47b7f780	MATH-859	Fix and then deprecate isSupportXxxInclusive in RealDistribution interface	The conclusion from [1] was never implemented. We should deprecate these properties from the RealDistribution interface, but since removal will have to wait until 4.0, we should agree on a precise definition and fix the code to match it in the mean time. The definition that I propose is that isSupportXxxInclusive means that when the density function is applied to the upper or lower bound of support returned by getSupportXxxBound, a finite (i.e. not infinite), not NaN value is returned. [1] http://markmail.org/message/dxuxh7eybl7xejde	https://issues.apache.org/jira/browse/MATH-859	src/main/java
23	src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java内protected UnivariatePointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java内protected UnivariatePointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java内protected UnivariatePointValuePair doOptimize()	2b4c1f87a73448323d21cd459f967013a41870ca	f1b04e990f4fc86544b6230fab1aa8ecfb74fbb1	MATH-855	"BrentOptimizer" not always reporting the best point	BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.	https://issues.apache.org/jira/browse/MATH-855	src/main/java
24	src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java内protected UnivariatePointValuePair doOptimize()外src/main/java/org/apache/commons/math3/optimization/univariate/BrentOptimizer.java内protected UnivariatePointValuePair doOptimize()	4e21bb6f6c4bc7380dc58bd8d23c99ea8f4ef5bd	d5ff460ba69e4261f066d7856e2f90b886924513	MATH-855	"BrentOptimizer" not always reporting the best point	BrentOptimizer (package "o.a.c.m.optimization.univariate") does not check that the point it is going to return is indeed the best one it has encountered. Indeed, the last evaluated point might be slightly worse than the one before last.	https://issues.apache.org/jira/browse/MATH-855	src/main/java
25	src/main/java/org/apache/commons/math3/optimization/fitting/HarmonicFitter.java内private void guessAOmega()	33201599dd578318ce0b87b1cd15c29a7d046097	996dd998e3081e4a842017b8ebcdae9b6059b530	MATH-844	"HarmonicFitter.ParameterGuesser" sometimes fails to return sensible values	The inner class "ParameterGuesser" in "HarmonicFitter" (package "o.a.c.m.optimization.fitting") fails to compute a usable guess for the "amplitude" parameter.	https://issues.apache.org/jira/browse/MATH-844	src/main/java
26	src/main/java/org/apache/commons/math3/fraction/Fraction.java内methodPartNotInFile:private Fraction(double value, double epsilon, int maxDenominator, int maxIterat外src/main/java/org/apache/commons/math3/fraction/Fraction.java内methodPartNotInFile:private Fraction(double value, double epsilon, int maxDenominator, int maxIterat	5f2077bb774d283e7984a2d5dc0c2759f2954963	1566dd339f6efc2347b0962fac7fce22adbc31ff	MATH-836	Fraction(double, int) constructor strange behaviour	The Fraction constructor Fraction(double, int) takes a double value and a int maximal denominator, and approximates a fraction. When the double value is a large, negative number with many digits in the fractional part, and the maximal denominator is a big, positive integer (in the 100'000s), two distinct bugs can manifest: 1: the constructor returns a positive Fraction. Calling Fraction(-33655.1677817278, 371880) returns the fraction 410517235/243036, which both has the wrong sign, and is far away from the absolute value of the given value 2: the constructor does not manage to reduce the Fraction properly. Calling Fraction(-43979.60679604749, 366081) returns the fraction -1651878166/256677, which should have* been reduced to -24654898/3831. I have, as of yet, not found a solution. The constructor looks like this: public Fraction(double value, int maxDenominator)         throws FractionConversionException     {        this(value, 0, maxDenominator, 100);     }  Increasing the 100 value (max iterations) does not fix the problem for all cases. Changing the 0-value (the epsilon, maximum allowed error) to something small does not work either, as this breaks the tests in FractionTest.  The problem is not neccissarily that the algorithm is unable to approximate a fraction correctly. A solution where a FractionConversionException had been thrown in each of these examples would probably be the best solution if an improvement on the approximation algorithm turns out to be hard to find. This bug has been found when trying to explore the idea of axiom-based testing (http://bldl.ii.uib.no/testing.html). Attached is a java test class FractionTestByAxiom (junit, goes into org.apache.commons.math3.fraction) which shows these bugs through a simplified approach to this kind of testing, and a text file describing some of the value/maxDenominator combinations which causes one of these failures.  It is never specified in the documentation that the Fraction class guarantees that completely reduced rational numbers are constructed, but a comment inside the equals method claims that "since fractions are always in lowest terms, numerators and can be compared directly for equality", so it seems like this is the intention.  	https://issues.apache.org/jira/browse/MATH-836	src/main/java
27	src/main/java/org/apache/commons/math3/fraction/Fraction.java内public Fraction divide(final int i)	d3fc5af31eb696af03cfbe2e18584c7e1d307d54	5f2077bb774d283e7984a2d5dc0c2759f2954963	MATH-835	Fraction percentageValue rare overflow	The percentageValue() method of the Fraction class works by first multiplying the Fraction by 100, then converting the Fraction to a double. This causes overflows when the numerator is greater than Integer.MAX_VALUE/100, even when the value of the fraction is far below this value. The patch changes the method to first convert to a double value, and then multiply this value by 100 - the result should be the same, but with less overflows. An addition to the test for the method that covers this bug is also included.	https://issues.apache.org/jira/browse/MATH-835	src/main/java
28	src/main/java/org/apache/commons/math3/optimization/linear/SimplexSolver.java内private Integer getPivotRow(SimplexTableau tableau, final int col)外src/main/java/org/apache/commons/math3/optimization/linear/SimplexSolver.java内private Integer getPivotRow(SimplexTableau tableau, final int col)外src/main/java/org/apache/commons/math3/optimization/linear/SimplexSolver.java内private Integer getPivotRow(SimplexTableau tableau, final int col)外src/main/java/org/apache/commons/math3/optimization/linear/SimplexSolver.java内private Integer getPivotRow(SimplexTableau tableau, final int col)	a55c951a3d573464e3905de7328fcdc487eebfb4	d3fc5af31eb696af03cfbe2e18584c7e1d307d54	MATH-828	Not expected UnboundedSolutionException	SimplexSolver throws UnboundedSolutionException when trying to solve minimization linear programming problem. The number of exception thrown depends on the number of variables. In order to see that behavior of SimplexSolver first try to run JUnit test setting a final variable ENTITIES_COUNT = 2 and that will give almost good result and then set it to 15 and you'll get a massive of unbounded exceptions. First iteration is runned with predefined set of input data with which the Solver gives back an appropriate result. The problem itself is well tested by it's authors (mathematicians who I believe know what they developed) using Matlab 10 with no unbounded solutions on the same rules of creatnig random variables values. What is strange to me is the dependence of the number of UnboundedSolutionException exceptions on the number of variables in the problem. The problem is formulated as min(1*t + 0*L) (for every r-th subject) s.t. -q(r) + QL >= 0 x(r)t - XL >= 0 L >= 0 where  r = 1..R,  L =  {l(1), l(2), ..., l(R)}  (vector of R rows and 1 column), Q - coefficients matrix MxR X - coefficients matrix NxR 	https://issues.apache.org/jira/browse/MATH-828	src/main/java
29	src/main/java/org/apache/commons/math3/linear/OpenMapRealVector.java内public OpenMapRealVector ebeDivide(RealVector v)外src/main/java/org/apache/commons/math3/linear/OpenMapRealVector.java内public OpenMapRealVector ebeMultiply(RealVector v)	350b2b1aa1ed5d7df05a77bf13f701ac1712dbc0	7bbddc2203bed78fafe7739a97df1f53e767341a	MATH-803	Bugs in RealVector.ebeMultiply(RealVector) and ebeDivide(RealVector)	OpenMapRealVector.ebeMultiply(RealVector) and OpenMapRealVector.ebeDivide(RealVector) return wrong values when one entry of the specified RealVector is nan or infinity. The bug is easy to understand. Here is the current implementation of ebeMultiply      public OpenMapRealVector ebeMultiply(RealVector v) {         checkVectorDimensions(v.getDimension());         OpenMapRealVector res = new OpenMapRealVector(this);         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             res.setEntry(iter.key(), iter.value() * v.getEntry(iter.key()));         }         return res;     }   The assumption is that for any double x, x * 0d == 0d holds, which is not true. The bug is easy enough to identify, but more complex to solve. The only solution I can come up with is to loop through all entries of v (instead of those entries which correspond to non-zero entries of this). I'm afraid about performance losses. 	https://issues.apache.org/jira/browse/MATH-803	src/main/java
30	src/main/java/org/apache/commons/math3/stat/inference/MannWhitneyUTest.java内private double calculateAsymptoticPValue(final double Umin, final int n1, final int n2)	631c5bcbb2d8d46eed1091031310ed24fcd866ed	a25e7f7abe7f6b3f4147febee4a917ce92241aab	MATH-790	Mann-Whitney U Test Suffers From Integer Overflow With Large Data Sets	When performing a Mann-Whitney U Test on large data sets (the attached test uses two 1500 element sets), intermediate integer values used in calculateAsymptoticPValue can overflow, leading to invalid results, such as p-values of NaN, or incorrect calculations. Attached is a patch, including a test, and a fix, which modifies the affected code to use doubles	https://issues.apache.org/jira/browse/MATH-790	src/main/java
31	src/main/java/org/apache/commons/math3/util/ContinuedFraction.java内public double evaluate(double x, double epsilon, int maxIterations)外src/main/java/org/apache/commons/math3/util/ContinuedFraction.java内public double evaluate(double x, double epsilon, int maxIterations)外src/main/java/org/apache/commons/math3/util/ContinuedFraction.java内public double evaluate(double x, double epsilon, int maxIterations)	6a5ef0149e123d9de19910d59e45d0e66bfd665c	2d846db3aec18dd081e680be05f6e0faad1cb186	MATH-718	inverseCumulativeProbability of BinomialDistribution returns wrong value for large trials.	The inverseCumulativeProbability method of the BinomialDistributionImpl class returns wrong value for large trials.  Following code will be reproduce the problem. System.out.println(new BinomialDistributionImpl(1000000, 0.5).inverseCumulativeProbability(0.5)); This returns 499525, though it should be 499999. I'm not sure how it should be fixed, but the cause is that the cumulativeProbability method returns Infinity, not NaN.  As the result the checkedCumulativeProbability method doesn't work as expected.	https://issues.apache.org/jira/browse/MATH-718	src/main/java
32	src/main/java/org/apache/commons/math3/geometry/euclidean/twod/PolygonsSet.java内protected void computeGeometricalProperties()	8d6478346e45d728656ccac760adc8126b3d225c	f81847d8079753ce45b049813730055188a22efb	MATH-780	BSPTree class and recovery of a Euclidean 3D BRep	New to the work here. Thanks for your efforts on this code. I create a BSPTree from a BoundaryRep (Brep) my test Brep is a cube as represented by a float array containing 8 3D points in(x,y,z) order and an array of indices (12 triplets for the 12 faces of the cube). I construct a BSPMesh() as shown in the code below. I can construct the PolyhedronsSet() but have problems extracting the faces from the BSPTree to reconstruct the BRep. The attached code (BSPMesh2.java) shows that a small change to 1 of the vertex positions causes/corrects the problem. Any ideas?	https://issues.apache.org/jira/browse/MATH-780	src/main/java
33	src/main/java/org/apache/commons/math3/optimization/linear/SimplexTableau.java内protected void dropPhase1Objective()	e5c7e4055ab2a40cb107a7ebd33766867325d5cc	3ef79313597d7d49067c48d65a09042d8d94822d	MATH-781	SimplexSolver gives bad results	Methode SimplexSolver.optimeze(...) gives bad results with commons-math3-3.0 in a simple test problem. It works well in commons-math-2.2. 	https://issues.apache.org/jira/browse/MATH-781	src/main/java
34	src/main/java/org/apache/commons/math3/genetics/ListPopulation.java内public String toString()	b91b3efb3854c0fd0f270bdba6cdfa8058c0515a	520f36b43a13681db338a07ac6c251cbe1a7a6e5	MATH-779	ListPopulation Iterator allows you to remove chromosomes from the population.	Calling the iterator method of ListPopulation returns an iterator of the protected modifiable list. Before returning the iterator we should wrap it in an unmodifiable list.	https://issues.apache.org/jira/browse/MATH-779	src/main/java
35	src/main/java/org/apache/commons/math3/genetics/ElitisticListPopulation.java内public ElitisticListPopulation(final List<Chromosome> chromosomes, final int populationLimit, final double elitismRate)外src/main/java/org/apache/commons/math3/genetics/ElitisticListPopulation.java内public ElitisticListPopulation(final List<Chromosome> chromosomes, final int populationLimit, final double elitismRate)	88f944e1aada536e9a70b041387bb328d61490cd	a5d8d115cb85149c62059a23e8f4d318cac0e903	MATH-776	Need range checks for elitismRate in ElitisticListPopulation constructors.	There is a range check for setting the elitismRate via ElitisticListPopulation's setElitismRate method, but not via the constructors.	https://issues.apache.org/jira/browse/MATH-776	src/main/java
36	src/main/java/org/apache/commons/math/fraction/BigFraction.java内public BigFraction divide(final BigFraction fraction)外src/main/java/org/apache/commons/math/fraction/BigFraction.java内public boolean equals(final Object other)	966d2eeef17b0b9bc7c07822be8da337bffedf97	1a15d5f4c13eca0435b0ed7e6a624064e7f7e07f	MATH-744	BigFraction.doubleValue() returns Double.NaN for large numerators or denominators	The current implementation of doubleValue() divides numerator.doubleValue() / denominator.doubleValue().  BigInteger.doubleValue() fails for any number greater than Double.MAX_VALUE.  So if the user has 308-digit numerator or denominator, the resulting quotient fails, even in cases where the result would be well inside Double's range. I have a patch to fix it, if I can figure out how to attach it here I will.	https://issues.apache.org/jira/browse/MATH-744	src/main/java
37	src/main/java/org/apache/commons/math/complex/Complex.java内public Complex sqrt1z()外src/main/java/org/apache/commons/math/complex/Complex.java内public Complex tan()	de147e5cc14f8dac6da785289421a55b11d21aa6	65ed08e15af15617e967d3ea9d635dc55a0ef866	MATH-722	[math] Complex Tanh for "big" numbers	Hi, In Complex.java the tanh is computed with the following formula: tanh(a + bi) = sinh(2a)/(cosh(2a)+cos(2b)) + [sin(2b)/(cosh(2a)+cos(2b))]i The problem that I'm finding is that as soon as "a" is a "big" number, both sinh(2a) and cosh(2a) are infinity and then the method tanh returns in the real part NaN (infinity/infinity) when it should return 1.0. Wouldn't it be appropiate to add something as in the FastMath library??: if (real>20.0){       return createComplex(1.0, 0.0); } if (real<-20.0){       return createComplex(-1.0, 0.0); } Best regards, JBB	https://issues.apache.org/jira/browse/MATH-722	src/main/java
38	src/main/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizer.java内private void prelim(double[] lowerBound, double[] upperBound)外src/main/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizer.java内private void prelim(double[] lowerBound, double[] upperBound)	74e00296574dc3ac0bc064fc3258faabaf732d6c	91cc42ba0493938aa53585720b315b62c5784a96	MATH-728	Errors in BOBYQAOptimizer when numberOfInterpolationPoints is greater than 2*dim+1	I've been having trouble getting BOBYQA to minimize a function (actually a non-linear least squares fit) so as one change I increased the number of interpolation points.  It seems that anything larger than 2*dim+1 causes an error (typically at line 1662                    interpolationPoints.setEntry(nfm, ipt, interpolationPoints.getEntry(ipt, ipt)); I'm guessing there is an off by one error in the translation from FORTRAN.  Changing the BOBYQAOptimizerTest as follows (increasing number of interpolation points by one) will cause failures. Bruce Index: src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java =================================================================== — src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java (revision 1221065) +++ src/test/java/org/apache/commons/math/optimization/direct/BOBYQAOptimizerTest.java (working copy) @@ -258,7 +258,7 @@  //        RealPointValuePair result = optim.optimize(100000, func, goal, startPoint);          final double[] lB = boundaries == null ? null : boundaries[0];          final double[] uB = boundaries == null ? null : boundaries[1];  BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 1); +        BOBYQAOptimizer optim = new BOBYQAOptimizer(2 * dim + 2);          RealPointValuePair result = optim.optimize(maxEvaluations, func, goal, startPoint, lB, uB);  //        System.out.println(func.getClass().getName() + " = "   //              + optim.getEvaluations() + " f(");  	https://issues.apache.org/jira/browse/MATH-728	src/main/java
39	src/main/java/org/apache/commons/math/ode/nonstiff/EmbeddedRungeKuttaIntegrator.java内public void integrate(final ExpandableStatefulODE equations, final double t)	c23859f0172edba1ff75577d0ad98e3fb41aa6d4	74e00296574dc3ac0bc064fc3258faabaf732d6c	MATH-727	too large first step with embedded Runge-Kutta integrators (Dormand-Prince 8(5,3) ...)	Adaptive step size integrators compute the first step size by themselves if it is not provided. For embedded Runge-Kutta type, this step size is not checked against the integration range, so if the integration range is extremely short, this step size may evaluate the function out of the range (and in fact it tries afterward to go back, and fails to stop). Gragg-Bulirsch-Stoer integrators do not have this problem, the step size is checked and truncated if needed.	https://issues.apache.org/jira/browse/MATH-727	src/main/java
40	src/main/java/org/apache/commons/math/analysis/solvers/BracketingNthOrderBrentSolver.java内protected double doSolve()	efa7825880d8c1c7411e51c5e21df6a004b78a3b	c8becc7c47963bcdc1578298846ad6fbf08f64ef	MATH-716	BracketingNthOrderBrentSolver exceeds maxIterationCount while updating always the same boundary	In some cases, the aging feature in BracketingNthOrderBrentSolver fails. It attempts to balance the bracketing points by targeting a non-zero value instead of the real root. However, the chosen target is too close too zero, and the inverse polynomial approximation is always on the same side, thus always updates the same bracket. In the real used case for a large program, I had a bracket point xA = 12500.0, yA = 3.7e-16, agingA = 0, which is the (really good) estimate of the zero on one side of the root and xB = 12500.03, yB = -7.0e-5, agingB = 97. This shows that the bracketing interval is completely unbalanced, and we never succeed to rebalance it as we always updates (xA, yA) and never updates (xB, yB).	https://issues.apache.org/jira/browse/MATH-716	src/main/java
41	src/main/java/org/apache/commons/math/stat/descriptive/moment/Variance.java内public double evaluate(final double[] values, final double[] weights, final int begin, final int length)	91e6ac584320681913de4a71a4e8d9f837e099b4	882556eabbeb2f62939aee29afdec2a01ce4bbe1	MATH-704	One of Variance.evaluate() methods does not work correctly	The method org.apache.commons.math.stat.descriptive.moment.Variance.evaluate(double[] values, double[] weights, double mean, int begin, int length) does not work properly. Looks loke it ignores the length parameter and grabs the whole dataset. Similar method in Mean class seems to work. I did not check other methods taking the part of the array; they may have the same problem. Workaround: I had to shrink my arrays and use the method without the length.	https://issues.apache.org/jira/browse/MATH-704	src/main/java
42	src/main/java/org/apache/commons/math/optimization/linear/SimplexTableau.java内protected RealPointValuePair getSolution()	800943ecf2e3ecf9258d8fdf8052e9273b816a97	e98a5000cd211539bf4ba65f62cc7f81395e1726	MATH-713	Negative value with restrictNonNegative	Problem: commons-math-2.2 SimplexSolver. A variable with 0 coefficient may be assigned a negative value nevertheless restrictToNonnegative flag in call: SimplexSolver.optimize(function, constraints, GoalType.MINIMIZE, true); Function 1 * x + 1 * y + 0 Constraints: 1 * x + 0 * y = 1 Result: x = 1; y = -1; Probably variables with 0 coefficients are omitted at some point of computation and because of that the restrictions do not affect their values.	https://issues.apache.org/jira/browse/MATH-713	src/main/java
43	src/main/java/org/apache/commons/math/stat/descriptive/SummaryStatistics.java内public void addValue(double value)	c282a044d713d78cbf7ff86694c2c224fb47e14f	dcae84b2e8f025e93340307d8bc04d406202c323	MATH-691	Statistics.setVarianceImpl makes getStandardDeviation produce NaN	Invoking SummaryStatistics.setVarianceImpl(new Variance(true/false) makes getStandardDeviation produce NaN. The code to reproduce it:  int[] scores = {1, 2, 3, 4}; SummaryStatistics stats = new SummaryStatistics(); stats.setVarianceImpl(new Variance(false)); //use "population variance" for(int i : scores) {   stats.addValue(i); } double sd = stats.getStandardDeviation(); System.out.println(sd);   A workaround suggested by Mikkel is:    double sd = FastMath.sqrt(stats.getSecondMoment() / stats.getN());  	https://issues.apache.org/jira/browse/MATH-691	src/main/java
44	src/main/java/org/apache/commons/math/ode/AbstractIntegrator.java内protected double acceptStep(final AbstractStepInterpolator interpolator, final double[] y, final double[] yDot, final double tEnd)外src/main/java/org/apache/commons/math/ode/AbstractIntegrator.java内public int compare(EventState es0, EventState es1)外src/main/java/org/apache/commons/math/ode/AbstractIntegrator.java内public int compare(EventState es0, EventState es1)	4d994702cb50c8ad80f4f977292063a118cff724	53c8cec5ceb4cd81e3f9b8858814accac83a324e	MATH-695	Incomplete reinitialization with some events handling	I get a bug with event handling: I track 2 events that occur in the same step, when the first one is accepted, it resets the state but the reinitialization is not complete and the second one becomes unable to find its way. I can't give my context, which is rather large, but I tried a patch that works for me, unfortunately it breaks the unit tests.	https://issues.apache.org/jira/browse/MATH-695	src/main/java
45	src/main/java/org/apache/commons/math/linear/OpenMapRealMatrix.java内noLeftCurtyIn1stLine:	4a702dbf7e2264c0b9757178f3afbde38e07fe41	bc4e9db01c2a03062965fa4bac65782376ab2287	MATH-679	Integer overflow in OpenMapRealMatrix	computeKey() has an integer overflow. Since it is a sparse matrix, this is quite easily encountered long before heap space is exhausted. The attached code demonstrates the problem, which could potentially be a security vulnerability (for example, if one was to use this matrix to store access control information). Workaround: never create an OpenMapRealMatrix with more cells than are addressable with an int.	https://issues.apache.org/jira/browse/MATH-679	src/main/java
46	src/main/java/org/apache/commons/math/complex/Complex.java内public Complex divide(Complex divisor)外src/main/java/org/apache/commons/math/complex/Complex.java内public Complex divide(double divisor)	330f3fe17d132bd4e2a91ff812ccf489e77f390f	e6f27ebcb9ee0a344308382b99a3894bb61b225d	MATH-657	Division by zero	In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.	https://issues.apache.org/jira/browse/MATH-657	src/main/java
47	src/main/java/org/apache/commons/math/complex/Complex.java内noLeftCurtyIn1stLine:外src/main/java/org/apache/commons/math/complex/Complex.java内public Complex(double real, double imaginary)外src/main/java/org/apache/commons/math/complex/Complex.java内public Complex divide(Complex divisor)外src/main/java/org/apache/commons/math/complex/Complex.java内public Complex divide(double divisor)	bbb5e1e198f995eddb393f820ed059aa774871c3	330f3fe17d132bd4e2a91ff812ccf489e77f390f	MATH-657	Division by zero	In class Complex, division by zero always returns NaN. I think that it should return NaN only when the numerator is also ZERO, otherwise the result should be INF. See here.	https://issues.apache.org/jira/browse/MATH-657	src/main/java
48	src/main/java/org/apache/commons/math/analysis/solvers/BaseSecantSolver.java内protected final double doSolve()	2af72281fcb919dac92b0c4e464f847adda23be1	13d22f45624470ce5c07c085cf8b1ec0251eaee6	MATH-631	"RegulaFalsiSolver" failure	The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.	https://issues.apache.org/jira/browse/MATH-631	src/main/java
49	src/main/java/org/apache/commons/math/linear/OpenMapRealVector.java内public double dotProduct(RealVector v)外src/main/java/org/apache/commons/math/linear/OpenMapRealVector.java内public OpenMapRealVector ebeDivide(RealVector v)外src/main/java/org/apache/commons/math/linear/OpenMapRealVector.java内public OpenMapRealVector ebeDivide(double[] v)外src/main/java/org/apache/commons/math/linear/OpenMapRealVector.java内public OpenMapRealVector ebeMultiply(RealVector v)	0b78d9accafadffe00697c2c56706043cdba5e64	09e35881f1ad74335707e70aa78fd347e37d1066	MATH-645	MathRuntimeException with simple ebeMultiply on OpenMapRealVector	The following piece of code  import org.apache.commons.math.linear.OpenMapRealVector; import org.apache.commons.math.linear.RealVector;  public class DemoBugOpenMapRealVector {     public static void main(String[] args) {         final RealVector u = new OpenMapRealVector(3, 1E-6);         u.setEntry(0, 1.);         u.setEntry(1, 0.);         u.setEntry(2, 2.);         final RealVector v = new OpenMapRealVector(3, 1E-6);         v.setEntry(0, 0.);         v.setEntry(1, 3.);         v.setEntry(2, 0.);         System.out.println(u);         System.out.println(v);         System.out.println(u.ebeMultiply(v));     } }   raises an exception  org.apache.commons.math.linear.OpenMapRealVector@7170a9b6 Exception in thread "main" org.apache.commons.math.MathRuntimeException$6: map has been modified while iterating  at org.apache.commons.math.MathRuntimeException.createConcurrentModificationException(MathRuntimeException.java:373)  at org.apache.commons.math.util.OpenIntToDoubleHashMap$Iterator.advance(OpenIntToDoubleHashMap.java:564)  at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:372)  at org.apache.commons.math.linear.OpenMapRealVector.ebeMultiply(OpenMapRealVector.java:1)  at DemoBugOpenMapRealVector.main(DemoBugOpenMapRealVector.java:17)  	https://issues.apache.org/jira/browse/MATH-645	src/main/java
50	src/main/java/org/apache/commons/math/analysis/solvers/BaseSecantSolver.java内protected final double doSolve()	ef028845113aca7e1159b1725edd7c39ed686faf	39cf5e69259d7560d50553caf028f9229b721013	MATH-631	"RegulaFalsiSolver" failure	The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.	https://issues.apache.org/jira/browse/MATH-631	src/main/java
51	src/main/java/org/apache/commons/math/analysis/solvers/BaseSecantSolver.java内protected final double doSolve()	34fef656d03a5ba75047a55a894f6f72cbe59f2e	2f066a5b2d2fe8a00a251a3220b0d52446fe392d	MATH-631	"RegulaFalsiSolver" failure	The following unit test:  @Test public void testBug() {     final UnivariateRealFunction f = new UnivariateRealFunction() {             @Override             public double value(double x) {                 return Math.exp(x) - Math.pow(Math.PI, 3.0);             }         };      UnivariateRealSolver solver = new RegulaFalsiSolver();     double root = solver.solve(100, f, 1, 10); }   fails with  illegal state: maximal count (100) exceeded: evaluations   Using "PegasusSolver", the answer is found after 17 evaluations.	https://issues.apache.org/jira/browse/MATH-631	src/main/java
52	src/main/java/org/apache/commons/math/geometry/euclidean/threed/Rotation.java内public Rotation(Vector3D u1, Vector3D u2, Vector3D v1, Vector3D v2)外src/main/java/org/apache/commons/math/geometry/euclidean/threed/Rotation.java内public Rotation(Vector3D u1, Vector3D u2, Vector3D v1, Vector3D v2)	def9fbf8701afa6937fe582c7572c7be011c319f	3c319db494928e9d51ea6091b301302c65f4eceb	MATH-639	numerical problems in rotation creation	building a rotation from the following vector pairs leads to NaN: u1 = -4921140.837095533, -2.1512094250440013E7, -890093.279426377 u2 = -2.7238580938724895E9, -2.169664921341876E9, 6.749688708885301E10 v1 = 1, 0, 0 v2 = 0, 0, 1 The constructor first changes the (v1, v2) pair into (v1', v2') ensuring the following scalar products hold:  <v1'|v1'> == <u1|u1>  <v2'|v2'> == <u2|u2>  <u1 |u2>  == <v1'|v2'> Once the (v1', v2') pair has been computed, we compute the cross product:   k = (v1' - u1)^(v2' - u2) and the scalar product:   c = <k | (u1^u2)> By construction, c is positive or null and the quaternion axis we want to build is q = k/[2*sqrt(c)]. c should be null only if some of the vectors are aligned, and this is dealt with later in the algorithm. However, there are numerical problems with the vector above with the way these computations are done, as shown by the following comparisons, showing the result we get from our Java code and the result we get from manual computation with the same formulas but with enhanced precision: commons math:   k = 38514476.5,            -84.,                           -1168590144 high precision: k = 38514410.36093388...,  -0.374075245201180409222711..., -1168590152.10599715208... and it becomes worse when computing c because the vectors are almost orthogonal to each other, hence inducing additional cancellations. We get: commons math    c = -1.2397173627587605E20 high precision: c =  558382746168463196.7079627... We have lost ALL significant digits in cancellations, and even the sign is wrong!	https://issues.apache.org/jira/browse/MATH-639	src/main/java
53	src/main/java/org/apache/commons/math/complex/Complex.java内public double abs()	3eb3573c62df5b89cb02a4ea67f08d85d2ceefba	7707b0bb80be05bbf6533a36bb0c646cbfd1026d	MATH-618	Complex Add and Subtract handle NaN arguments differently, but javadoc contracts are the same	For both Complex add and subtract, the javadoc states that       * If either this or <code>rhs</code> has a NaN value in either part,      * {@link #NaN} is returned; otherwise Inifinite and NaN values are      * returned in the parts of the result according to the rules for      * {@link java.lang.Double} arithmetic   Subtract includes an isNaN test and returns Complex.NaN if either complex argument isNaN; but add omits this test.  The test should be added to the add implementation (actually restored, since this looks like a code merge problem going back to 1.1).	https://issues.apache.org/jira/browse/MATH-618	src/main/java
54	src/main/java/org/apache/commons/math/dfp/Dfp.java内protected Dfp(final DfpField field, double x)外src/main/java/org/apache/commons/math/dfp/Dfp.java内public double toDouble()	804309e5c5726ad22b0c74dfccdb1ed318f0a108	c00ac8120a4215125e49775cd9351e689586b467	MATH-567	class Dfp toDouble method return -inf whan Dfp value is 0 "zero"	I found a bug in the toDouble() method of the Dfp class. If the Dfp's value is 0 "zero", the toDouble() method returns a  negative infini. This is because the double value returned has an exposant equal to 0xFFF  and a significand is equal to 0. In the IEEE754 this is a -inf. To be equal to zero, the exposant and the significand must be equal to zero. A simple test case is : ---------------------------------------------- import org.apache.commons.math.dfp.DfpField; public class test {  /**  @param args   */  public static void main(String[] args)  {   DfpField field = new DfpField(100);   System.out.println("toDouble value of getZero() ="+field.getZero().toDouble()+     "\ntoDouble value of newDfp(0.0) ="+     field.newDfp(0.0).toDouble());  } }  May be the simplest way to fix it is to test the zero equality at the begin of the toDouble() method, to be able to return the correctly signed zero ?	https://issues.apache.org/jira/browse/MATH-567	src/main/java
55	src/main/java/org/apache/commons/math/geometry/Vector3D.java内public static double dotProduct(Vector3D v1, Vector3D v2)外src/main/java/org/apache/commons/math/geometry/Vector3D.java内public static Vector3D crossProduct(final Vector3D v1, final Vector3D v2)	3114d4ed4654f54fc1ee8e3c3da5b6cd1cd2cc87	0c38504ffa3ef520ce78e260538d7b8742c895e8	MATH-554	Vector3D.crossProduct is sensitive to numerical cancellation	Cross product implementation uses the naive formulas (y1 z2 - y2 z1, ...). These formulas fail when vectors are almost colinear, like in the following example:  Vector3D v1 = new Vector3D(9070467121.0, 4535233560.0, 1); Vector3D v2 = new Vector3D(9070467123.0, 4535233561.0, 1); System.out.println(Vector3D.crossProduct(v1, v2));   The previous code displays  { -1, 2, 0 }  instead of the correct answer  { -1, 2, 1 }	https://issues.apache.org/jira/browse/MATH-554	src/main/java
56	src/main/java/org/apache/commons/math/util/MultidimensionalCounter.java内public int getDimension()	d773adb1228299c05ae9abbacf2f3616e9a170e0	e06fe05e2dd68936e770ba67caa7b9924568170d	MATH-552	MultidimensionalCounter.getCounts(int) returns wrong array of indices	MultidimensionalCounter counter = new MultidimensionalCounter(2, 4); for (Integer i : counter) {     int[] x = counter.getCounts;     System.out.println(i + " " + Arrays.toString); } Output is: 0 [0, 0] 1 [0, 1] 2 [0, 2] 3 [0, 2]   <=== should be [0, 3] 4 [1, 0] 5 [1, 1] 6 [1, 2] 7 [1, 2]   <=== should be [1, 3]	https://issues.apache.org/jira/browse/MATH-552	src/main/java
57	src/main/java/org/apache/commons/math/stat/clustering/KMeansPlusPlusClusterer.java内methodPartNotInFile:public KMeansPlusPlusClusterer(final Random random, final EmptyClusterStrategy e	d3892cb85f3eb8ccadada228b791170e3e0a9124	00fea9d8078d487e31cec8292dbd9bd69bc9c216	MATH-546	Truncation issue in KMeansPlusPlusClusterer	The for loop inside KMeansPlusPlusClusterer.chooseInitialClusters defines a variable   int sum = 0; This variable should have type double, rather than int.  Using an int causes the method to truncate the distances between points to (square roots of) integers.  It's especially bad when the distances between points are typically less than 1. As an aside, in version 2.2, this bug manifested itself by making the clusterer return empty clusters.  I wonder if the EmptyClusterStrategy would still be necessary if this bug were fixed.	https://issues.apache.org/jira/browse/MATH-546	src/main/java
58	src/main/java/org/apache/commons/math/optimization/fitting/GaussianFitter.java内public double value(double x, double[] p)	b1392325d1255f7e3a1b04a12859a8bb75e4d14c	45add3a0e7e2e94bfc29e85c9ef0856e2e473a33	MATH-519	GaussianFitter Unexpectedly Throws NotStrictlyPositiveException	Running the following:      double[] observations =   {         1.1143831578403364E-29,          4.95281403484594E-28,          1.1171347211930288E-26,          1.7044813962636277E-25,          1.9784716574832164E-24,          1.8630236407866774E-23,          1.4820532905097742E-22,          1.0241963854632831E-21,          6.275077366673128E-21,          3.461808994532493E-20,          1.7407124684715706E-19,          8.056687953553974E-19,          3.460193945992071E-18,          1.3883326374011525E-17,          5.233894983671116E-17,          1.8630791465263745E-16,          6.288759227922111E-16,          2.0204433920597856E-15,          6.198768938576155E-15,          1.821419346860626E-14,          5.139176445538471E-14,          1.3956427429045787E-13,          3.655705706448139E-13,          9.253753324779779E-13,          2.267636001476696E-12,          5.3880460095836855E-12,          1.2431632654852931E-11       } ;      GaussianFitter g =        new GaussianFitter(new LevenbergMarquardtOptimizer());      for (int index = 0; index < 27; index++)      {       g.addObservedPoint(index, observations[index]);      }         g.fit(); Results in: org.apache.commons.math.exception.NotStrictlyPositiveException: -1.277 is smaller than, or equal to, the minimum (0)  at org.apache.commons.math.analysis.function.Gaussian$Parametric.validateParameters(Gaussian.java:184)  at org.apache.commons.math.analysis.function.Gaussian$Parametric.value(Gaussian.java:129) I'm guessing the initial guess for sigma is off.  	https://issues.apache.org/jira/browse/MATH-519	src/main/java
59	src/main/java/org/apache/commons/math/util/FastMath.java内public static long max(final long a, final long b)	ffcaeb072fe0789bb9a64f8488ce9df742ba7da9	5dcca48038fb6274cc155251d09db12746ccce71	MATH-482	FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f	FastMath.max(50.0f, -50.0f) => -50.0f; should be +50.0f. This is because the wrong variable is returned. The bug was not detected by the test case "testMinMaxFloat()" because that has a bug too - it tests doubles, not floats.	https://issues.apache.org/jira/browse/MATH-482	src/main/java
60	src/main/java/org/apache/commons/math/distribution/NormalDistributionImpl.java内public double density(double x)	87430634d19f8f3af1e0019f733afb578c80d669	9054aac4b948117b838d6a5b15be1f50965d805a	MATH-414	ConvergenceException in NormalDistributionImpl.cumulativeProbability()	I get a ConvergenceException in  NormalDistributionImpl.cumulativeProbability() for very large/small parameters including Infinity, -Infinity. For instance in the following code:  @Test  public void testCumulative() {   final NormalDistribution nd = new NormalDistributionImpl();   for (int i = 0; i < 500; i++) {    final double val = Math.exp;    try  {     System.out.println("val = " + val + " cumulative = " + nd.cumulativeProbability(val));    }  catch (MathException e)  {     e.printStackTrace();     fail();    }   }  } In version 2.0, I get no exception.  My suggestion is to change in the implementation of cumulativeProbability(double) to catch all ConvergenceException (and return for very large and very small values), not just MaxIterationsExceededException.	https://issues.apache.org/jira/browse/MATH-414	src/main/java
61	src/main/java/org/apache/commons/math/distribution/PoissonDistributionImpl.java内noLeftCurtyIn1stLine:外src/main/java/org/apache/commons/math/distribution/PoissonDistributionImpl.java内public PoissonDistributionImpl(double p)	30bacfbe14e57ec64d08baae52efa91877e8dd9b	f76efe5f4ef36fadc677c94269927076f2f42eb9	MATH-349	Dangerous code in "PoissonDistributionImpl"	In the following excerpt from class "PoissonDistributionImpl": PoissonDistributionImpl.java     public PoissonDistributionImpl(double p, NormalDistribution z) {         super();         setNormal(z);         setMean(p);     }   (1) Overridable methods are called within the constructor. (2) The reference "z" is stored and modified within the class. I've encountered problem (1) in several classes while working on issue 348. In those cases, in order to remove potential problems, I copied/pasted the body of the "setter" methods inside the constructor but I think that a more elegant solution would be to remove the "setters" altogether (i.e. make the classes immutable). Problem (2) can also create unexpected behaviour. Is it really necessary to pass the "NormalDistribution" object; can't it be always created within the class?	https://issues.apache.org/jira/browse/MATH-349	src/main/java
62	src/main/java/org/apache/commons/math/optimization/univariate/MultiStartUnivariateRealOptimizer.java内public UnivariateRealPointValuePair optimize(final FUNC f, final GoalType goal, final double min, final double max)外src/main/java/org/apache/commons/math/optimization/univariate/MultiStartUnivariateRealOptimizer.java内public UnivariateRealPointValuePair optimize(final FUNC f, final GoalType goal, final double min, final double max, final double startValue)	6a07726a825da09003d153e52ea30005b1a4c013	8de3eb542b2be80ba309b2c91ae1dba75bdb5063	MATH-413	Miscellaneous issues concerning the "optimization" package	Revision 990792 contains changes triggered the following issues:  MATH-394 MATH-397 MATH-404  This issue collects the currently still unsatisfactory code (not necessarily sorted in order of annoyance):  "BrentOptimizer": a specific convergence checker must be used. "LevenbergMarquardtOptimizer" also has specific convergence checks. Trying to make convergence checking independent of the optimization algorithm creates problems (conceptual and practical):   See "BrentOptimizer" and "LevenbergMarquardtOptimizer", the algorithm passes "points" to the convergence checker, but the actual meaning of the points can very well be different in the caller (optimization algorithm) and the callee (convergence checker). In "PowellOptimizer" the line search ("BrentOptimizer") tolerances depend on the tolerances within the main algorithm. Since tolerances come with "ConvergenceChecker" and so can be changed at any time, it is awkward to adapt the values within the line search optimizer without exposing its internals ("BrentOptimizer" field) to the enclosing class ("PowellOptimizer").   Given the numerous changes, some Javadoc comments might be out-of-sync, although I did try to update them all. Class "DirectSearchOptimizer" (in package "optimization.direct") inherits from class "AbstractScalarOptimizer" (in package "optimization.general"). Some interfaces are defined in package "optimization" but their base implementations (abstract class that contain the boiler-plate code) are in package "optimization.general" (e.g. "DifferentiableMultivariateVectorialOptimizer" and "BaseAbstractVectorialOptimizer"). No check is performed to ensure the the convergence checker has been set (see e.g. "BrentOptimizer" and "PowellOptimizer"); if it hasn't there will be a NPE. The alternative is to initialize a default checker that will never be used in case the user had intended to explicitly sets the checker. "NonLinearConjugateGradientOptimizer": Ugly workaround for the checked "ConvergenceException". Everywhere, we trail the checked "FunctionEvaluationException" although it is never used. There remains some duplicate code (such as the "multi-start loop" in the various "MultiStart..." implementations). The "ConvergenceChecker" interface is very general (the "converged" method can take any number of "...PointValuePair"). However there remains a "semantic" problem: One cannot be sure that the list of points means the same thing for the caller of "converged" and within the implementation of the "ConvergenceChecker" that was independently set. It is not clear whether it is wise to aggregate the counter of gradient evaluations to the function evaluation counter. In "LevenbergMarquartdOptimizer" for example, it would be unfair to do so. Currently I had to remove all tests referring to gradient and Jacobian evaluations. In "AbstractLeastSquaresOptimizer" and "LevenbergMarquardtOptimizer", occurences of "OptimizationException" were replaced by the unchecked "ConvergenceException" but in some cases it might not be the most appropriate one. "MultiStartUnivariateRealOptimizer": in the other classes ("MultiStartMultivariate...") similar to this one, the randomization is on the firts-guess value while in this class, it is on the search interval. I think that here also we should randomly choose the start value (within the user-selected interval). The Javadoc utility raises warnings (see output of "mvn site") which I couldn't figure out how to correct. Some previously existing classes and interfaces have become no more than a specialisation of new "generics" classes; it might be interesting to remove them in order to reduce the number of classes and thus limit the potential for confusion.  	https://issues.apache.org/jira/browse/MATH-413	src/main/java
63	src/main/java/org/apache/commons/math/util/MathUtils.java内public static double cosh(double x)	91b44a1ef7563f54e8acdf29906bd872985a2a6e	d2a5bc02c002acfa220ce8bad23b9e8af137f47d	MATH-370	NaN in "equals" methods	In "MathUtils", some "equals" methods will return true if both argument are NaN. Unless I'm mistaken, this contradicts the IEEE standard. If nobody objects, I'm going to make the changes.	https://issues.apache.org/jira/browse/MATH-370	src/main/java
64	src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()	1909dbe0ca1fc678d9baf1388f8efcf4c65b2a54	7dadc2ab019f066a7a287376ad4c63193e8a0a9a	MATH-405	Inconsistent result from Levenberg-Marquardt	Levenberg-Marquardt (its method doOptimize) returns a VectorialPointValuePair.  However, the class holds the optimum point, the vector of the objective function, the cost and residuals.  The value returns by doOptimize does not always corresponds to the point which leads to the residuals and cost	https://issues.apache.org/jira/browse/MATH-405	src/main/java
65	src/main/java/org/apache/commons/math/optimization/general/AbstractLeastSquaresOptimizer.java内protected void updateResidualsAndCost()外src/main/java/org/apache/commons/math/optimization/general/AbstractLeastSquaresOptimizer.java内public double getChiSquare()	d4e5f1ce10a4ca8c13625bd03c00a156238655cc	a4f9188a55e6935d4d38ae97806af4d59e671588	MATH-377	weight versus sigma in AbstractLeastSquares	In AbstractLeastSquares, residualsWeights contains the WEIGHTS assigned to each observation.  In the method getRMS(), these weights are multiplicative as they should. unlike in getChiSquare() where it appears at the denominator!   If the weight is really the weight of the observation, it should multiply the square of the residual even in the computation of the chi2.  Once corrected, getRMS() can even reduce  public double getRMS()  {return Math.sqrt(getChiSquare()/rows);}	https://issues.apache.org/jira/browse/MATH-377	src/main/java
66	src/main/java/org/apache/commons/math/optimization/univariate/BrentOptimizer.java内noLeftCurtyIn1stLine:外src/main/java/org/apache/commons/math/optimization/univariate/BrentOptimizer.java内public BrentOptimizer()外src/main/java/org/apache/commons/math/optimization/univariate/BrentOptimizer.java内methodPartNotInFile:public double optimize(final UnivariateRealFunction f, final GoalType goalType,外src/main/java/org/apache/commons/math/optimization/univariate/BrentOptimizer.java内private double localMin(boolean isMinim, UnivariateRealFunction f, GoalType goalType, double lo, double mid, double hi, double eps, double t)外src/main/java/org/apache/commons/math/optimization/univariate/BrentOptimizer.java内private double localMin(boolean isMinim, UnivariateRealFunction f, GoalType goalType, double lo, double mid, double hi, double eps, double t)外src/main/java/org/apache/commons/math/optimization/univariate/BrentOptimizer.java内private double localMin(boolean isMinim, UnivariateRealFunction f, GoalType goalType, double lo, double mid, double hi, double eps, double t)	7bbad398dd4da51c933414be9a17b07179dee5e4	76fcbc838c0f27d9b029c1f283390cb4f47f8895	MATH-395	Bugs in "BrentOptimizer"	I apologize for having provided a buggy implementation of Brent's optimization algorithm (class "BrentOptimizer" in package "optimization.univariate"). The unit tests didn't show that there was something wrong, although (from the "changes.xml" file) I discovered that, at the time, Luc had noticed something weird in the implementation's behaviour. Comparing with an implementation in Python, I could figure out the fixes. I'll modify "BrentOptimizer" and add a test. I also propose to change the name of the unit test class from "BrentMinimizerTest" to "BrentOptimizerTest".	https://issues.apache.org/jira/browse/MATH-395	src/main/java
67	src/main/java/org/apache/commons/math/optimization/MultiStartUnivariateRealOptimizer.java内methodPartNotInFile:public MultiStartUnivariateRealOptimizer(final UnivariateRealOptimizer optimizer	28f28f18ae42f154e9c43de8c3b78f85adf9b318	fc505bba569dede0d29a401798fa9f55df9e27f1	MATH-393	Method "getResult()" in "MultiStartUnivariateRealOptimizer"	In "MultiStartUnivariateRealOptimizer" (package "optimization"), the method "getResult" returns the result of the last run of the "underlying" optimizer; this last result might not be the best one, in which case it will not correspond to the value returned by the "optimize" method. This is confusing and does not seem very useful. I think that "getResult" should be defined as    public double getResult() {     return optima[0]; }   and similarly  public double getFunctionValue() {     return optimaValues[0]; }  	https://issues.apache.org/jira/browse/MATH-393	src/main/java
68	src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内public LevenbergMarquardtOptimizer()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()外src/main/java/org/apache/commons/math/optimization/general/LevenbergMarquardtOptimizer.java内protected VectorialPointValuePair doOptimize()	103f12391b89112f030b921a7c4969f00ff23b44	615ca9a000c253575e6f62bed87db6110b750834	MATH-362	LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it	LevenbergMarquardtOptimizer ignores the VectorialConvergenceChecker parameter passed to it. This makes it hard to specify custom stopping criteria for the optimizer.	https://issues.apache.org/jira/browse/MATH-362	src/main/java
69	src/main/java/org/apache/commons/math/stat/correlation/PearsonsCorrelation.java内public RealMatrix getCorrelationPValues()	7272826424a28e22d96b86a8e320a37b9e006e71	a2711c189d9754c22e3aba2de1c6c125e52626aa	MATH-371	PearsonsCorrelation.getCorrelationPValues() precision limited by machine epsilon	Similar to the issue described in MATH-201, using PearsonsCorrelation.getCorrelationPValues() with many treatments results in p-values that are continuous down to 2.2e-16 but that drop to 0 after that. In MATH-201, the problem was described as such: > So in essence, the p-value returned by TTestImpl.tTest() is: >  > 1.0 - (cumulativeProbability(t) - cumulativeProbabily(-t)) >  > For large-ish t-statistics, cumulativeProbabilty(-t) can get quite small, and cumulativeProbabilty(t) can get very close to 1.0. When  > cumulativeProbability(-t) is less than the machine epsilon, we get p-values equal to zero because: >  > 1.0 - 1.0 + 0.0 = 0.0 The solution in MATH-201 was to modify the p-value calculation to this: > p = 2.0 * cumulativeProbability(-t) Here, the problem is similar.  From PearsonsCorrelation.getCorrelationPValues():   p = 2 * (1 - tDistribution.cumulativeProbability(t)); Directly calculating the p-value using identical code as PearsonsCorrelation.getCorrelationPValues(), but with the following change seems to solve the problem:   p = 2 * (tDistribution.cumulativeProbability(-t)); 	https://issues.apache.org/jira/browse/MATH-371	src/main/java
70	src/main/java/org/apache/commons/math/analysis/solvers/BisectionSolver.java内public double solve(double min, double max)	f184aeb7810bed8c89b2c8cca4f8164aef535e56	f41fcd85ca62a2109a6e550be0353d292d351213	MATH-369	BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial) throws NullPointerException	Method      BisectionSolver.solve(final UnivariateRealFunction f, double min, double max, double initial)   invokes      BisectionSolver.solve(double min, double max)  which throws NullPointerException, as member variable     UnivariateRealSolverImpl.f  is null. Instead the method:     BisectionSolver.solve(final UnivariateRealFunction f, double min, double max) should be called. Steps to reproduce: invoke:      new BisectionSolver().solve(someUnivariateFunctionImpl, 0.0, 1.0, 0.5); NullPointerException will be thrown. 	https://issues.apache.org/jira/browse/MATH-369	src/main/java
71	src/main/java/org/apache/commons/math/ode/nonstiff/EmbeddedRungeKuttaIntegrator.java内public double integrate(final FirstOrderDifferentialEquations equations, final double t0, final double[] y0, final double t, final double[] y)外src/main/java/org/apache/commons/math/ode/nonstiff/RungeKuttaIntegrator.java内public double integrate(final FirstOrderDifferentialEquations equations, final double t0, final double[] y0, final double t, final double[] y)	fa959ca8c89133c3a2f024a020ec80948ee31926	6537e18ad189603e19615226e6aa1a9cdd154b8f	MATH-358	ODE integrator goes past specified end of integration range	End of integration range in ODE solving is handled as an event. In some cases, numerical accuracy in events detection leads to error in events location. The following test case shows the end event is not handled properly and an integration that should cover a 60s range in fact covers a 160s range, more than twice the specified range.    public void testMissedEvent() throws IntegratorException, DerivativeException {           final double t0 = 1878250320.0000029;           final double t =  1878250379.9999986;           FirstOrderDifferentialEquations ode = new FirstOrderDifferentialEquations() {                          public int getDimension() {                 return 1;             }                          public void computeDerivatives(double t, double[] y, double[] yDot)                 throws DerivativeException {                 yDot[0] = y[0] * 1.0e-6;             }         };          DormandPrince853Integrator integrator = new DormandPrince853Integrator(0.0, 100.0,                                                                                1.0e-10, 1.0e-10);          double[] y = { 1.0 };         integrator.setInitialStepSize(60.0);         double finalT = integrator.integrate(ode, t0, y, t, y);         Assert.assertEquals(t, finalT, 1.0e-6);     }   	https://issues.apache.org/jira/browse/MATH-358	src/main/java
72	src/main/java/org/apache/commons/math/analysis/solvers/BrentSolver.java内public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)外src/main/java/org/apache/commons/math/analysis/solvers/BrentSolver.java内public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)	29c3b75e2d5120771ca85ffcbffc9b7cb5d45c58	aa09ac7ca6e01a2ba41470ca590f07f2ada8af6f	MATH-344	Brent solver returns the wrong value if either bracket endpoint is root	The solve(final UnivariateRealFunction f, final double min, final double max, final double initial) function returns yMin or yMax if min or max are deemed to be roots, respectively, instead of min or max.	https://issues.apache.org/jira/browse/MATH-344	src/main/java
73	src/main/java/org/apache/commons/math/analysis/solvers/BrentSolver.java内public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)	1687372e34828dff42e1482ed6575bb982b81b00	29c3b75e2d5120771ca85ffcbffc9b7cb5d45c58	MATH-343	Brent solver doesn't throw IllegalArgumentException when initial guess has the wrong sign	Javadoc for "public double solve(final UnivariateRealFunction f, final double min, final double max, final double initial)" claims that "if the values of the function at the three points have the same sign" an IllegalArgumentException is thrown. This case isn't even checked.	https://issues.apache.org/jira/browse/MATH-343	src/main/java
74	src/main/java/org/apache/commons/math/ode/nonstiff/EmbeddedRungeKuttaIntegrator.java内public double integrate(final FirstOrderDifferentialEquations equations, final double t0, final double[] y0, final double t, final double[] y)	034b4d68157defc3199f618fdd2d3a487306dbc2	d06e7b7c8e646ff489a5dfba47dfcb283a194e9b	MATH-338	Wrong parameter for first step size guess for Embedded Runge Kutta methods	In a space application using DOP853 i detected what seems to be a bad parameter in the call to the method  initializeStep of class AdaptiveStepsizeIntegrator. Here, DormandPrince853Integrator is a subclass for EmbeddedRungeKuttaIntegrator which perform the call to initializeStep at the beginning of its method integrate(...) The problem comes from the array "scale" that is used as a parameter in the call off initializeStep(..) Following the theory described by Hairer in his book "Solving Ordinary Differential Equations 1 : Nonstiff Problems", the scaling should be : sci = Atol i + |y0i| * Rtoli Whereas EmbeddedRungeKuttaIntegrator uses :  sci = Atoli Note that the Gragg-Bulirsch-Stoer integrator uses the good implementation "sci = Atol i + |y0i| * Rtoli  " when he performs the call to the same method initializeStep(..) In the method initializeStep, the error leads to a wrong step size h used to perform an  Euler step. Most of the time it is unvisible for the user. But in my space application the Euler step with this wrong step size h (much bigger than it should be)  makes an exception occur (my satellite hits the ground...) To fix the bug, one should use the same algorithm as in the rescale method in GraggBulirschStoerIntegrator For exemple :  final double[] scale= new double[y0.length];;           if (vecAbsoluteTolerance == null) {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = scalAbsoluteTolerance + scalRelativeTolerance * yi;               }             } else {               for (int i = 0; i < scale.length; ++i)  {                 final double yi = Math.max(Math.abs(y0[i]), Math.abs(y0[i]));                 scale[i] = vecAbsoluteTolerance[i] + vecRelativeTolerance[i] * yi;               }             }           hNew = initializeStep(equations, forward, getOrder(), scale,                            stepStart, y, yDotK[0], yTmp, yDotK[1]); Sorry for the length of this message, looking forward to hearing from you soon Vincent Morand 	https://issues.apache.org/jira/browse/MATH-338	src/main/java
75	src/main/java/org/apache/commons/math/stat/Frequency.java内public long getCount(char v)	39e2ad8af8fdedac51144158061cb31e7863c3a1	523df0c13719150b1397a6094f410274bfaf1475	MATH-329	In stat.Frequency, getPct(Object) uses getCumPct(Comparable) instead of getPct(Comparable)	Drop in Replacement of 1.2 with 2.0 not possible because all getPct calls will be cummulative without code change Frequency.java    /**  Returns the percentage of values that are equal to v @deprecated replaced by  {@link #getPct(Comparable)}  as of 2.0      */     @Deprecated     public double getPct(Object v)  {         return getCumPct((Comparable<?>) v);     }  	https://issues.apache.org/jira/browse/MATH-329	src/main/java
76	src/main/java/org/apache/commons/math/linear/SingularValueDecompositionImpl.java内public RealMatrix getU()外src/main/java/org/apache/commons/math/linear/SingularValueDecompositionImpl.java内public RealMatrix getV()	d607614a8827a1a5fab587b6e692601d5673833e	39e2ad8af8fdedac51144158061cb31e7863c3a1	MATH-320	NaN singular value from SVD	The following jython code Start code from org.apache.commons.math.linear import * Alist = [[1.0, 2.0, 3.0],[2.0,3.0,4.0],[3.0,5.0,7.0]] A = Array2DRowRealMatrix(Alist) decomp = SingularValueDecompositionImpl(A) print decomp.getSingularValues() End code prints array('d', [11.218599757513008, 0.3781791648535976, nan]) The last singular value should be something very close to 0 since the matrix is rank deficient.  When i use the result from getSolver() to solve a system, i end  up with a bunch of NaNs in the solution.  I assumed i would get back a least squares solution. Does this SVD implementation require that the matrix be full rank?  If so, then i would expect an exception to be thrown from the constructor or one of the methods. 	https://issues.apache.org/jira/browse/MATH-320	src/main/java
77	src/main/java/org/apache/commons/math/linear/OpenMapRealVector.java内public double getL1Distance(double[] v)外src/main/java/org/apache/commons/math/linear/ArrayRealVector.java内public double getL1Norm()	b30bff0126e1a0301a8b86b40a0b532f1d0ae28f	d6555de715889237b7be11639d164e7098862003	MATH-326	getLInfNorm() uses wrong formula in both ArrayRealVector and OpenMapRealVector (in different ways)	the L_infinity norm of a finite dimensional vector is just the max of the absolute value of its entries. The current implementation in ArrayRealVector has a typo:      public double getLInfNorm() {         double max = 0;         for (double a : data) {             max += Math.max(max, Math.abs(a));         }         return max;     }   the += should just be an =. There is sadly a unit test assuring us that this is the correct behavior (effectively a regression-only test, not a test for correctness). Worse, the implementation in OpenMapRealVector is not even positive semi-definite:          public double getLInfNorm() {         double max = 0;         Iterator iter = entries.iterator();         while (iter.hasNext()) {             iter.advance();             max += iter.value();         }         return max;     }   I would suggest that this method be moved up to the AbstractRealVector superclass and implemented using the sparseIterator():    public double getLInfNorm() {     double norm = 0;     Iterator<Entry> it = sparseIterator();     Entry e;     while(it.hasNext() && (e = it.next()) != null) {       norm = Math.max(norm, Math.abs(e.getValue()));     }     return norm;   }   Unit tests with negative valued vectors would be helpful to check for this kind of thing in the future.	https://issues.apache.org/jira/browse/MATH-326	src/main/java
78	src/main/java/org/apache/commons/math/ode/events/EventState.java内public boolean evaluateStep(final StepInterpolator interpolator)外src/main/java/org/apache/commons/math/ode/events/EventState.java内public boolean evaluateStep(final StepInterpolator interpolator)	cc7e3e7fe46844f068c622cf439ef4b654d971ec	b3ed2415fc58b34667d99f077bae6c8e38b7aced	MATH-322	during ODE integration, the last event in a pair of very close event may not be detected	When an events follows a previous one very closely, it may be ignored. The occurrence of the bug depends on the side of the bracketing interval that was selected. For example consider a switching function that is increasing around first event around t = 90, reaches its maximum and is decreasing around the second event around t = 135. If an integration step spans from 67.5 and 112.5, the switching function values at start and end of step will  have opposite signs, so the first event will be detected. The solver will find the event really occurs at 90.0 and will therefore truncate the step at 90.0. The next step will start from where the first step ends, i.e. it will start at 90.0. Let's say this step spans from 90.0 to 153.0. The switching function switches once again in this step. If the solver for the first event converged to a value slightly before 90.0 (say 89.9999999), then the switch will not be detected because g(89.9999999) and g(153.0) are both negative. This bug was introduced as of r781157 (2009-06-02) when special handling of events very close to step start was added.	https://issues.apache.org/jira/browse/MATH-322	src/main/java
79	src/main/java/org/apache/commons/math/util/MathUtils.java内public static double distance(double[] p1, double[] p2)	ca1ca9250b99dd49434468aa5530eb00b2f0680d	138baec1c778c2ea1dc1a6efe6d5e76a3b072b1d	MATH-305	NPE in  KMeansPlusPlusClusterer unittest	When running this unittest, I am facing this NPE: java.lang.NullPointerException  at org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer.assignPointsToClusters(KMeansPlusPlusClusterer.java:91) This is the unittest: package org.fao.fisheries.chronicles.calcuation.cluster; import static org.junit.Assert.assertEquals; import static org.junit.Assert.assertTrue; import java.util.Arrays; import java.util.List; import java.util.Random; import org.apache.commons.math.stat.clustering.Cluster; import org.apache.commons.math.stat.clustering.EuclideanIntegerPoint; import org.apache.commons.math.stat.clustering.KMeansPlusPlusClusterer; import org.fao.fisheries.chronicles.input.CsvImportProcess; import org.fao.fisheries.chronicles.input.Top200Csv; import org.junit.Test; public class ClusterAnalysisTest {  @Test  public void testPerformClusterAnalysis2() {   KMeansPlusPlusClusterer<EuclideanIntegerPoint> transformer = new KMeansPlusPlusClusterer<EuclideanIntegerPoint>(     new Random(1746432956321l));   EuclideanIntegerPoint[] points = new EuclideanIntegerPoint[] {     new EuclideanIntegerPoint(new int[]  { 1959, 325100 } ),     new EuclideanIntegerPoint(new int[]  { 1960, 373200 } ), };   List<Cluster<EuclideanIntegerPoint>> clusters = transformer.cluster(Arrays.asList(points), 1, 1);   assertEquals(1, clusters.size());  } }	https://issues.apache.org/jira/browse/MATH-305	src/main/java
80	src/main/java/org/apache/commons/math/linear/EigenDecompositionImpl.java内private int goodStep(final int start, final int end)	7d903ca533833e5db8d2fe150f1e31b54ddda700	3ced4f2a4e2546f0d7c309bd8f6305edee0dee8f	MATH-318	wrong result in eigen decomposition	Some results computed by EigenDecompositionImpl are wrong. The following case computed by Fortran Lapack fails with version 2.0      public void testMathpbx02() {          double[] mainTridiagonal = {            7484.860960227216, 18405.28129035345, 13855.225609560746,           10016.708722343366, 559.8117399576674, 6750.190788301587,               71.21428769782159         };         double[] secondaryTridiagonal = {           -4175.088570476366,1975.7955858241994,5193.178422374075,             1995.286659169179,75.34535882933804,-234.0808002076056         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {           20654.744890306974412,16828.208208485466457,           6893.155912634994820,6757.083016675340332,           5887.799885688558788,64.309089923240379,           57.992628792736340         };         RealVector[] refEigenVectors = {           new ArrayRealVector(new double[] {-0.270356342026904, 0.852811091326997, 0.399639490702077, 0.198794657813990, 0.019739323307666, 0.000106983022327, -0.000001216636321}),           new ArrayRealVector(new double[] {0.179995273578326,-0.402807848153042,0.701870993525734,0.555058211014888,0.068079148898236,0.000509139115227,-0.000007112235617}),           new ArrayRealVector(new double[] {-0.399582721284727,-0.056629954519333,-0.514406488522827,0.711168164518580,0.225548081276367,0.125943999652923,-0.004321507456014}),           new ArrayRealVector(new double[] {0.058515721572821,0.010200130057739,0.063516274916536,-0.090696087449378,-0.017148420432597,0.991318870265707,-0.034707338554096}),           new ArrayRealVector(new double[] {0.855205995537564,0.327134656629775,-0.265382397060548,0.282690729026706,0.105736068025572,-0.009138126622039,0.000367751821196}),           new ArrayRealVector(new double[] {-0.002913069901144,-0.005177515777101,0.041906334478672,-0.109315918416258,0.436192305456741,0.026307315639535,0.891797507436344}),           new ArrayRealVector(new double[] {-0.005738311176435,-0.010207611670378,0.082662420517928,-0.215733886094368,0.861606487840411,-0.025478530652759,-0.451080697503958})         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-3);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-5);             }         }      }  	https://issues.apache.org/jira/browse/MATH-318	src/main/java
81	src/main/java/org/apache/commons/math/linear/EigenDecompositionImpl.java内private void computeGershgorinCircles()外src/main/java/org/apache/commons/math/linear/EigenDecompositionImpl.java内private void processGeneralBlock(final int n)外src/main/java/org/apache/commons/math/linear/EigenDecompositionImpl.java内methodPartNotInFile:private void computeShiftIncrement(final int start, final int end, final int def	6e3ea6857dce8b7a586c3af555e21ba35a59ea54	7d903ca533833e5db8d2fe150f1e31b54ddda700	MATH-308	ArrayIndexOutOfBoundException in EigenDecompositionImpl	The following test triggers an ArrayIndexOutOfBoundException:      public void testMath308() {          double[] mainTridiagonal = {             22.330154644539597, 46.65485522478641, 17.393672330044705, 54.46687435351116, 80.17800767709437         };         double[] secondaryTridiagonal = {             13.04450406501361, -5.977590941539671, 2.9040909856707517, 7.1570352792841225         };          // the reference values have been computed using routine DSTEMR         // from the fortran library LAPACK version 3.2.1         double[] refEigenValues = {             14.138204224043099, 18.847969733754262, 52.536278520113882, 53.456697699894512, 82.044413207204002         };         RealVector[] refEigenVectors = {             new ArrayRealVector(new double[] {  0.584677060845929, -0.367177264979103, -0.721453187784497,  0.052971054621812, -0.005740715188257 }),             new ArrayRealVector(new double[] {  0.713933751051495, -0.190582113553930,  0.671410443368332, -0.056056055955050,  0.006541576993581 }),             new ArrayRealVector(new double[] {  0.222368839324646,  0.514921891363332, -0.021377019336614,  0.801196801016305, -0.207446991247740 }),             new ArrayRealVector(new double[] {  0.314647769490148,  0.750806415553905, -0.167700312025760, -0.537092972407375,  0.143854968127780 }),             new ArrayRealVector(new double[] { -0.000462690386766, -0.002118073109055,  0.011530080757413,  0.252322434584915,  0.967572088232592 })         };          // the following line triggers the exception         EigenDecomposition decomposition =             new EigenDecompositionImpl(mainTridiagonal, secondaryTridiagonal, MathUtils.SAFE_MIN);          double[] eigenValues = decomposition.getRealEigenvalues();         for (int i = 0; i < refEigenValues.length; ++i) {             assertEquals(refEigenValues[i], eigenValues[i], 1.0e-6);             if (refEigenVectors[i].dotProduct(decomposition.getEigenvector(i)) < 0) {                 assertEquals(0, refEigenVectors[i].add(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             } else {                 assertEquals(0, refEigenVectors[i].subtract(decomposition.getEigenvector(i)).getNorm(), 1.0e-6);             }         }      }   Running the previous method as a Junit test triggers the exception when the EigenDecompositionImpl instance is built. The first few lines of the stack trace are:  java.lang.ArrayIndexOutOfBoundsException: -1  at org.apache.commons.math.linear.EigenDecompositionImpl.computeShiftIncrement(EigenDecompositionImpl.java:1545)  at org.apache.commons.math.linear.EigenDecompositionImpl.goodStep(EigenDecompositionImpl.java:1072)  at org.apache.commons.math.linear.EigenDecompositionImpl.processGeneralBlock(EigenDecompositionImpl.java:894)  at org.apache.commons.math.linear.EigenDecompositionImpl.findEigenvalues(EigenDecompositionImpl.java:658)  at org.apache.commons.math.linear.EigenDecompositionImpl.decompose(EigenDecompositionImpl.java:246)  at org.apache.commons.math.linear.EigenDecompositionImpl.<init>(EigenDecompositionImpl.java:205)  at org.apache.commons.math.linear.EigenDecompositionImplTest.testMath308(EigenDecompositionImplTest.java:136)   I'm currently investigating this bug. It is not a simple index translation error between the original fortran (Lapack) and commons-math implementation.	https://issues.apache.org/jira/browse/MATH-308	src/main/java
82	src/main/java/org/apache/commons/math/optimization/linear/SimplexSolver.java内private Integer getPivotRow(final int col, final SimplexTableau tableau)	f36be8e8b7371fb8814456b6c095866aa802de78	4ece1884cc408e15c8f1db9010fec5cac43937b9	MATH-288	SimplexSolver not working as expected 2	SimplexSolver didn't find the optimal solution. Program for Lpsolve: ===================== /* Objective function */ max: 7 a 3 b; /* Constraints */ R1: +3 a -5 c <= 0; R2: +2 a -5 d <= 0; R3: +2 b -5 c <= 0; R4: +3 b -5 d <= 0; R5: +3 a +2 b <= 5; R6: +2 a +3 b <= 5; /* Variable bounds */ a <= 1; b <= 1; ===================== Results(correct): a = 1, b = 1, value = 10 Program for SimplexSolve: ===================== LinearObjectiveFunction kritFcia = new LinearObjectiveFunction(new double[] {7, 3, 0, 0} , 0); Collection<LinearConstraint> podmienky = new ArrayList<LinearConstraint>(); podmienky.add(new LinearConstraint(new double[] {1, 0, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {0, 1, 0, 0} , Relationship.LEQ, 1)); podmienky.add(new LinearConstraint(new double[] {3, 0, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {2, 0, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 2, -5, 0} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {0, 3, 0, -5} , Relationship.LEQ, 0)); podmienky.add(new LinearConstraint(new double[] {3, 2, 0, 0} , Relationship.LEQ, 5)); podmienky.add(new LinearConstraint(new double[] {2, 3, 0, 0} , Relationship.LEQ, 5)); SimplexSolver solver = new SimplexSolver(); RealPointValuePair result = solver.optimize(kritFcia, podmienky, GoalType.MAXIMIZE, true); ===================== Results(incorrect): a = 1, b = 0.5, value = 8.5 P.S. I used the latest software from the repository (including MATH-286 fix).	https://issues.apache.org/jira/browse/MATH-288	src/main/java
83	src/main/java/org/apache/commons/math/optimization/linear/SimplexTableau.java内private Integer getBasicRowForSolution(final int col)外src/main/java/org/apache/commons/math/optimization/linear/SimplexTableau.java内private void copyArray(final double[] src, final double[] dest, final int destPos)	0878ad4bc8e4ac345a2c27449a0686b0877013d4	f36be8e8b7371fb8814456b6c095866aa802de78	MATH-286	SimplexSolver not working as expected?	I guess (but I could be wrong) that SimplexSolver does not always return the optimal solution, nor satisfies all the constraints... Consider this LP: max: 0.8 x0 + 0.2 x1 + 0.7 x2 + 0.3 x3 + 0.6 x4 + 0.4 x5; r1: x0 + x2 + x4 = 23.0; r2: x1 + x3 + x5 = 23.0; r3: x0 >= 10.0; r4: x2 >= 8.0; r5: x4 >= 5.0; LPSolve returns 25.8, with x0 = 10.0, x1 = 0.0, x2 = 8.0, x3 = 0.0, x4 = 5.0, x5 = 23.0; The same LP expressed in Apache commons math is: LinearObjectiveFunction f = new LinearObjectiveFunction(new double[]  { 0.8, 0.2, 0.7, 0.3, 0.6, 0.4 } , 0 ); Collection<LinearConstraint> constraints = new ArrayList<LinearConstraint>(); constraints.add(new LinearConstraint(new double[]  { 1, 0, 1, 0, 1, 0 } , Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[]  { 0, 1, 0, 1, 0, 1 } , Relationship.EQ, 23.0)); constraints.add(new LinearConstraint(new double[]  { 1, 0, 0, 0, 0, 0 } , Relationship.GEQ, 10.0)); constraints.add(new LinearConstraint(new double[]  { 0, 0, 1, 0, 0, 0 } , Relationship.GEQ, 8.0)); constraints.add(new LinearConstraint(new double[]  { 0, 0, 0, 0, 1, 0 } , Relationship.GEQ, 5.0)); RealPointValuePair solution = new SimplexSolver().optimize(f, constraints, GoalType.MAXIMIZE, true); that returns 22.20, with x0 = 15.0, x1 = 23.0, x2 = 8.0, x3 = 0.0, x4 = 0.0, x5 = 0.0; Is it possible SimplexSolver is buggy that way? The returned value is 22.20 instead of 25.8, and the last constraint (x4 >= 5.0) is not satisfied... Am I using the interface wrongly?	https://issues.apache.org/jira/browse/MATH-286	src/main/java
84	src/main/java/org/apache/commons/math/optimization/direct/MultiDirectional.java内public MultiDirectional(final double khi, final double gamma)外src/main/java/org/apache/commons/math/optimization/direct/MultiDirectional.java内protected void iterateSimplex(final Comparator<RealPointValuePair> comparator)	6eeb2b31a0e4b9690666f26d148d0eafe1f662e7	317793eeff84dc9b260301adbe4aa8f5c79f703a	MATH-283	MultiDirectional optimzation loops forver if started at the correct solution	MultiDirectional.iterateSimplex loops forever if the starting point is the correct solution. see the attached test case (testMultiDirectionalCorrectStart) as an example.	https://issues.apache.org/jira/browse/MATH-283	src/main/java
85	src/java/org/apache/commons/math/analysis/solvers/UnivariateRealSolverUtils.java内public static double solve(UnivariateRealFunction f, double x0, double x1, double absoluteAccuracy)	8de8a3b6fa5f4d7ea817f915d957ef309441762d	9be86f674f91fa3fe0a8694e7b98472dabe6886d	MATH-280	bug in inverseCumulativeProbability() for Normal Distribution	 @version $Revision: 617953 $ $Date: 2008-02-02 22:54:00 -0700 (Sat, 02 Feb 2008) $  */ public class NormalDistributionImpl extends AbstractContinuousDistribution    @version $Revision: 506600 $ $Date: 2007-02-12 12:35:59 -0700 (Mon, 12 Feb 2007) $  */ public abstract class AbstractContinuousDistribution  This code:          DistributionFactory factory = app.getDistributionFactory();          NormalDistribution normal = factory.createNormalDistribution(0,1);          double result = normal.inverseCumulativeProbability(0.9772498680518209); gives the exception below. It should return (approx) 2.0000... normal.inverseCumulativeProbability(0.977249868051820); works fine These also give errors: 0.9986501019683698 (should return 3.0000...) 0.9999683287581673 (should return 4.0000...) org.apache.commons.math.MathException: Number of iterations=1, maximum iterations=2,147,483,647, initial=1, lower bound=0, upper bound=179,769,313,486,231,570,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000, final a value=0, final b value=2, f(a)=-0.477, f(b)=0  at org.apache.commons.math.distribution.AbstractContinuousDistribution.inverseCumulativeProbability(AbstractContinuousDistribution.java:103)  at org.apache.commons.math.distribution.NormalDistributionImpl.inverseCumulativeProbability(NormalDistributionImpl.java:145) 	https://issues.apache.org/jira/browse/MATH-280	src/java
86	src/java/org/apache/commons/math/linear/CholeskyDecompositionImpl.java内public CholeskyDecompositionImpl(final RealMatrix matrix, final double relativeSymmetryThreshold, final double absolutePositivityThreshold)外src/java/org/apache/commons/math/linear/CholeskyDecompositionImpl.java内public CholeskyDecompositionImpl(final RealMatrix matrix, final double relativeSymmetryThreshold, final double absolutePositivityThreshold)	75f5c92aeb47e264c196a8c38a495adac89f493c	bd8f05c4dda4b5b00e525e08fd6c95d256423c97	MATH-274	testing for symmetric positive definite matrix in CholeskyDecomposition	I used this matrix:         double[][] cv = {  {0.40434286, 0.09376327, 0.30328980, 0.04909388} ,  {0.09376327, 0.10400408, 0.07137959, 0.04762857} ,  {0.30328980, 0.07137959, 0.30458776, 0.04882449},             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         };  And it works fine, because it is symmetric positive definite  I tried this matrix:          double[][] cv = {             {0.40434286, -0.09376327, 0.30328980, 0.04909388},             {-0.09376327, 0.10400408, 0.07137959, 0.04762857},             {0.30328980, 0.07137959, 0.30458776, 0.04882449} ,             {0.04909388, 0.04762857, 0.04882449, 0.07543265}         }; And it should throw an exception but it does not.  I tested the matrix in R and R's cholesky decomposition method returns that the matrix is not symmetric positive definite. Obviously your code is not catching this appropriately. By the way (in my opinion) the use of exceptions to check these conditions is not the best design or use for exceptions.  If you are going to force the use to try and catch these exceptions at least provide methods  to test the conditions prior to the possibility of the exception.   	https://issues.apache.org/jira/browse/MATH-274	src/java
87	src/java/org/apache/commons/math/optimization/linear/SimplexTableau.java内methodPartNotInFile:protected static double getInvertedCoeffiecientSum(final RealVector coefficients	d0c52c49b7efa5367b154443bba9910cb19e6419	75f5c92aeb47e264c196a8c38a495adac89f493c	MATH-273	Basic variable is not found correctly in simplex tableau	The last patch to SimplexTableau caused an automated test suite I'm running at work to go down a new code path and uncover what is hopefully the last bug remaining in the Simplex code. SimplexTableau was assuming an entry in the tableau had to be nonzero to indicate a basic variable, which is incorrect - the entry should have a value equal to 1.	https://issues.apache.org/jira/browse/MATH-273	src/java
88	src/java/org/apache/commons/math/optimization/linear/SimplexTableau.java内protected RealPointValuePair getSolution()	1b48dbabe18ca22396bd737df73e241cfdf8c0c5	24a6a2692e942969f5c39bd88d3d12ac0f0bf0d9	MATH-272	Simplex Solver arrives at incorrect solution	I have reduced the problem reported to me down to a minimal test case which I will attach.	https://issues.apache.org/jira/browse/MATH-272	src/java
89	src/java/org/apache/commons/math/stat/Frequency.java内public String toString()	0c84b28c3542f3340f7ec0dffc34c60dff66604e	62b3877f953dd47c4d301be35c77446e2cf55311	MATH-259	Bugs in Frequency API	I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.	https://issues.apache.org/jira/browse/MATH-259	src/java
90	src/java/org/apache/commons/math/stat/Frequency.java内public String toString()外src/java/org/apache/commons/math/stat/Frequency.java内public void addValue(Object v)	43336b08c6c27d55c1c6e8c1b6330cb44a29044e	0c84b28c3542f3340f7ec0dffc34c60dff66604e	MATH-259	Bugs in Frequency API	I think the existing Frequency API has some bugs in it. The addValue(Object v) method allows one to add a plain Object, but one cannot add anything further to the instance, as the second add fails with IllegalArgumentException. In fact, the problem is with the first call to addValue(Object) which should not allow a plain Object to be added - it should only allow Comparable objects. This could be fixed by checking that the object is Comparable. Similar considerations apply to the getCumFreq(Object) and getCumPct(Object) methods - they will only work with objects that implement Comparable. The getCount(Object) and getPct(Object) methods don't fail when given a non-Comparable object (because the class cast exception is caught), however they just return 0 as if the object was not present:          final Object OBJ = new Object();         f.addValue(OBJ); // This ought to fail, but doesn't, causing the unexpected behaviour below         System.out.println(f.getCount(OBJ)); // 0         System.out.println(f.getPct(OBJ)); // 0.0   Rather than adding extra checks for Comparable, it seems to me that the API would be much improved by using Comparable instead of Object. Also, it should make it easier to implement generics. However, this would cause compilation failures for some programs that pass Object rather than Comparable to the class. These would need recoding, but I think they would continue to run OK against the new API. It would also affect the run-time behaviour slightly, as the first attempt to add a non-Comparable object would fail, rather than the second add of a possibly valid object. But is that a viable program? It can only add one object, and any attempt to get statistics will either return 0 or an Exception, and applying the instanceof fix would also cause it to fail.	https://issues.apache.org/jira/browse/MATH-259	src/java
91	src/java/org/apache/commons/math/fraction/Fraction.java内public Fraction abs()	9fb5d27fd15d856b136fbadd3d7bb7dfbd0ddf50	e1df5f5486f10ac521dfc3dc20d1bfe508e67aa8	MATH-252	Fraction.comparTo returns 0 for some differente fractions	If two different fractions evaluate to the same double due to limited precision, the compareTo methode returns 0 as if they were identical.  // value is roughly PI - 3.07e-18 Fraction pi1 = new Fraction(1068966896, 340262731);  // value is roughly PI + 1.936e-17 Fraction pi2 = new Fraction( 411557987, 131002976);  System.out.println(pi1.doubleValue() - pi2.doubleValue()); // exactly 0.0 due to limited IEEE754 precision System.out.println(pi1.compareTo(pi2)); // display 0 instead of a negative value  	https://issues.apache.org/jira/browse/MATH-252	src/java
92	src/java/org/apache/commons/math/util/MathUtils.java内public static long binomialCoefficient(final int n, final int k)外src/java/org/apache/commons/math/util/MathUtils.java内public static long binomialCoefficient(final int n, final int k)外src/java/org/apache/commons/math/util/MathUtils.java内public static double binomialCoefficientLog(final int n, final int k)	63c7bc9d7cbee21a7f6acc5a9b85d1b8b0e3a205	85a8ab2dcfc26e83be1514acdb11d3feaac3e73d	MATH-241	MathUtils.binomialCoefficient(n,k) fails for large results	Probably due to rounding errors, MathUtils.binomialCoefficient(n,k) fails for results near Long.MAX_VALUE. The existence of failures can be demonstrated by testing the recursive property:           assertEquals(MathUtils.binomialCoefficient(65,32) + MathUtils.binomialCoefficient(65,33),                  MathUtils.binomialCoefficient(66,33));   Or by directly using the (externally calculated and hopefully correct) expected value:           assertEquals(7219428434016265740L, MathUtils.binomialCoefficient(66,33));   I suggest a nonrecursive test implementation along the lines of MathUtilsTest.java     /**      * Exact implementation using BigInteger and the explicit formula      * (n, k) == ((k-1)*...*n) / (1*...*(n-k))      */  public static long binomialCoefficient(int n, int k) {   if (k == 0 || k == n)    return 1;   BigInteger result = BigInteger.ONE;   for (int i = k + 1; i <= n; i++) {    result = result.multiply(BigInteger.valueOf(i));   }   for (int i = 1; i <= n - k; i++) {    result = result.divide(BigInteger.valueOf(i));   }   if (result.compareTo(BigInteger.valueOf(Long.MAX_VALUE)) > 0) {    throw new ArithmeticException(                                 "Binomial coefficient overflow: " + n + ", " + k);   }   return result.longValue();  }   Which would allow you to test the expected values directly:           assertEquals(binomialCoefficient(66,33), MathUtils.binomialCoefficient(66,33));  	https://issues.apache.org/jira/browse/MATH-241	src/java
93	src/java/org/apache/commons/math/util/MathUtils.java内public static boolean equals(double[] x, double[] y)外src/java/org/apache/commons/math/util/MathUtils.java内public static double factorialDouble(final int n)外src/java/org/apache/commons/math/util/MathUtils.java内public static double factorialLog(final int n)	3112f69e1d789e00fb21a1dde901b313547fed60	7cd3d822b65b7acdd1f6e1c82b291cd994f4fe05	MATH-240	MathUtils.factorial(n) fails for n >= 17	The result of MathUtils.factorial( n ) for n = 17, 18, 19 is wrong, probably because of rounding errors in the double calculations. Replace the first line of MathUtilsTest.testFactorial() by         for (int i = 1; i <= 20; i++) { to check all valid arguments for the long result and see the failure. I suggest implementing a simple loop to multiply the long result - or even using a precomputed long[] - instead of adding logarithms.	https://issues.apache.org/jira/browse/MATH-240	src/java
94	src/java/org/apache/commons/math/util/MathUtils.java内public static double factorialLog(final int n)	ee88057ba86b543c4f678f8e28b1620c22b19f0f	a96e597894562c2cf01fc678f6c435d65ccc31c5	MATH-238	MathUtils.gcd(u, v) fails when u and v both contain a high power of 2	The test at the beginning of MathUtils.gcd(u, v) for arguments equal to zero fails when u and v contain high enough powers of 2 so that their product overflows to zero.         assertEquals(3 * (1<<15), MathUtils.gcd(3 * (1<<20), 9 * (1<<15))); Fix: Replace the test at the start of MathUtils.gcd()         if (u * v == 0) { by         if (u == 0 || v == 0) {	https://issues.apache.org/jira/browse/MATH-238	src/java
95	src/java/org/apache/commons/math/distribution/FDistributionImpl.java内protected double getDomainUpperBound(double p)	e640d1613751a99ae4c468c9567f21ea13b496fc	fbf87122e0f7229892b6dbbf2e211cc46acea008	MATH-227	denominatorDegreeOfFreedom in FDistribution leads to IllegalArgumentsException in UnivariateRealSolverUtils.bracket	We are using the FDistributionImpl from the commons.math project to do some statistical calculations, namely receiving the upper and lower boundaries of a confidence interval. Everything is working fine and the results are matching our reference calculations. However, the FDistribution behaves strange if a denominatorDegreeOfFreedom of 2 is used, with an alpha-value of 0.95. This results in an IllegalArgumentsException, stating: Invalid endpoint parameters:  lowerBound=0.0 initial=Infinity upperBound=1.7976931348623157E308 coming from org.apache.commons.math.analysis.UnivariateRealSolverUtils.bracket The problem is the 'initial' parameter to that function, wich is POSITIVE_INFINITY and therefore not within the boundaries. I already pinned down the problem to the FDistributions getInitialDomain()-method, wich goes like:         return getDenominatorDegreesOfFreedom() /                     (getDenominatorDegreesOfFreedom() - 2.0); Obviously, in case of denominatorDegreesOfFreedom == 2, this must lead to a division-by-zero, resulting in POSTIVE_INFINITY. The result of this operation is then directly passed into the UnivariateRealSolverUtils.bracket() - method as second argument.	https://issues.apache.org/jira/browse/MATH-227	src/java
96	src/java/org/apache/commons/math/complex/Complex.java内public boolean equals(Object other)	704342d6280becc3be194a23dfda2e56c2a64aed	e6449cccdeaba96ffba4c27db322c5c3d5c18662	MATH-221	Result of multiplying and equals for complex numbers is wrong	Hi. The bug relates on complex numbers. The methods "multiply" and "equals" of the class Complex are involved. mathematic background:  (0,i) * (-1,0i) = (0,-i). little java program + output that shows the bug: -----------------------------------------------------------------------  import org.apache.commons.math.complex.*; public class TestProg {         public static void main(String[] args) {                  ComplexFormat f = new ComplexFormat();                 Complex c1 = new Complex(0,1);                 Complex c2 = new Complex(-1,0);                  Complex res = c1.multiply(c2);                 Complex comp = new Complex(0,-1);                  System.out.println("res:  "+f.format(res));                 System.out.println("comp: "+f.format(comp));                  System.out.println("res=comp: "+res.equals(comp));         } }   ----------------------------------------------------------------------- res:  -0 - 1i comp: 0 - 1i res=comp: false ----------------------------------------------------------------------- I think the "equals" should return "true". The problem could either be the "multiply" method that gives (-0,-1i) instead of (0,-1i), or if you think thats right, the equals method has to be modified. Good Luck Dieter	https://issues.apache.org/jira/browse/MATH-221	src/java
97	src/java/org/apache/commons/math/analysis/BrentSolver.java内methodPartNotInFile:public double solve(double min, double max) throws MaxIterationsExceededExceptio	7cf0c980d5d814daf187502bc07da0542ed7a828	ed492bd0c5c5c3a0258a65cb31cc8723d8f011fd	MATH-204	BrentSolver throws IllegalArgumentException	I am getting this exception: java.lang.IllegalArgumentException: Function values at endpoints do not have different signs.  Endpoints: [-100000.0,1.7976931348623157E308]  Values: [0.0,-101945.04630982173] at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:99) at org.apache.commons.math.analysis.BrentSolver.solve(BrentSolver.java:62) The exception should not be thrown with values  [0.0,-101945.04630982173] because 0.0 is positive. According to Brent Worden, the algorithm should stop and return 0 as the root instead of throwing an exception. The problem comes from this method:     public double solve(double min, double max) throws MaxIterationsExceededException,          FunctionEvaluationException {         clearResult();         verifyInterval(min, max);         double yMin = f.value(min);         double yMax = f.value(max);         // Verify bracketing         if (yMin * yMax >= 0)  {             throw new IllegalArgumentException             ("Function values at endpoints do not have different signs." +                     "  Endpoints: [" + min + "," + max + "]" +                      "  Values: [" + yMin + "," + yMax + "]");                }          // solve using only the first endpoint as initial guess         return solve(min, yMin, max, yMax, min, yMin);     } One way to fix it would be to add this code after the assignment of yMin and yMax:         if (yMin ==0 || yMax == 0)  {          return 0;         }	https://issues.apache.org/jira/browse/MATH-204	src/java
98	src/java/org/apache/commons/math/linear/BigMatrixImpl.java内public BigDecimal getTrace()外src/java/org/apache/commons/math/linear/RealMatrixImpl.java内public double getTrace()	26cf6d543deeb2e59564ba23568c83eda2b389bd	a0f3e0435c405ec842240413e2165863c9c76a16	MATH-209	RealMatrixImpl#operate gets result vector dimensions wrong	org.apache.commons.math.linear.RealMatrixImpl#operate tries to create a result vector that always has the same length as the input vector. This can result in runtime exceptions if the matrix is non-square and it always yields incorrect results if the matrix is non-square. The correct behaviour would of course be to create a vector with the same length as the row dimension of the matrix. Thus line 640 in RealMatrixImpl.java should read double[] out = new double[nRows]; instead of double[] out = new double[v.length];	https://issues.apache.org/jira/browse/MATH-209	src/java
99	src/java/org/apache/commons/math/util/MathUtils.java内public static int gcd(final int p, final int q)外src/java/org/apache/commons/math/util/MathUtils.java内public static int lcm(int a, int b)	924b6d76a2d44baf858c3aee46ab1439c9641959	58b5609fe8f99aabc990d885bf6c4d85631b7c79	MATH-243	MathUtils.gcd(Integer.MIN_VALUE, 0) should throw an Exception instead of returning Integer.MIN_VALUE	The gcd method should throw an Exception for gcd(Integer.MIN_VALUE, 0), like for gcd(Integer.MIN_VALUE, Integer.MIN_VALUE). The method should only return nonnegative results.	https://issues.apache.org/jira/browse/MATH-243	src/java
100	src/java/org/apache/commons/math/estimation/AbstractEstimator.java内public double getChiSquare(EstimationProblem problem)外src/java/org/apache/commons/math/estimation/AbstractEstimator.java内public double getChiSquare(EstimationProblem problem)	0e9b00010655cffb2bd0443cca5706588bfc628d	876d133334e8dde309cc11f884c0dd4cc9ce530e	MATH-200	AbstractEstimator: getCovariances() and guessParametersErrors() crash when having bound parameters	the two methods getCovariances() and guessParametersErrors() from org.apache.commons.math.estimation.AbstractEstimator crash with ArrayOutOfBounds exception when some of the parameters are bound. The reason is that the Jacobian is calculated only for the unbound parameters. in the code you loop through all parameters. line #166: final int cols = problem.getAllParameters().length; should be replaced by:  final int cols = problem.getUnboundParameters().length; (similar changes could be done in guessParametersErrors()) the dissadvantage of the above bug fix is that what is returned to the user is an array with smaller size than the number of all parameters. Alternatively, you can have some logic in the code which writes zeros for the elements of the covariance matrix corresponding to the bound parameters	https://issues.apache.org/jira/browse/MATH-200	src/java
101	src/java/org/apache/commons/math/complex/ComplexFormat.java内public Complex parse(String source, ParsePosition pos)	29b732ce0f974e2347b50477c401f0c503a8a981	32643d19538ad853a1280eb4060c4f15ac6dc3dd	MATH-198	java.lang.StringIndexOutOfBoundsException in ComplexFormat.parse(String source, ParsePosition pos)	The parse(String source, ParsePosition pos) method in the ComplexFormat class does not check whether the imaginary character is set or not which produces StringIndexOutOfBoundsException in the substring method : (line 375 of ComplexFormat) ...         // parse imaginary character         int n = getImaginaryCharacter().length();         startIndex = pos.getIndex();         int endIndex = startIndex + n;         if (source.substring(startIndex, endIndex).compareTo(             getImaginaryCharacter()) != 0) { ... I encoutered this exception typing in a JTextFied with ComplexFormat set to look up an AbstractFormatter. If only the user types the imaginary part of the complex number first, he gets this exception. Solution: Before setting to n length of the imaginary character, check if the source contains it. My proposal: ...         int n = 0;         if (source.contains(getImaginaryCharacter()))         n = getImaginaryCharacter().length(); ...    F.S.	https://issues.apache.org/jira/browse/MATH-198	src/java
102	src/java/org/apache/commons/math/stat/inference/ChiSquareTestImpl.java内public double chiSquare(double[] expected, long[] observed)	f9b16a4ae42164d096dd21cc060c0802039ba15a	a1bac127067c912b4a9f7a2957c427853a36c730	MATH-175	chiSquare(double[] expected, long[] observed) is returning incorrect test statistic	ChiSquareTestImpl is returning incorrect chi-squared value. An implicit assumption of public double chiSquare(double[] expected, long[] observed) is that the sum of expected and observed are equal. That is, in the code: for (int i = 0; i < observed.length; i++)  {             dev = ((double) observed[i] - expected[i]);             sumSq += dev * dev / expected[i];         } this calculation is only correct if sum(observed)==sum(expected). When they are not equal then one must rescale the expected value by sum(observed) / sum(expected) so that they are. Ironically, it is an example in the unit test ChiSquareTestTest that highlights the error: long[] observed1 =  { 500, 623, 72, 70, 31 } ;         double[] expected1 =  { 485, 541, 82, 61, 37 } ;         assertEquals( "chi-square test statistic", 16.4131070362, testStatistic.chiSquare(expected1, observed1), 1E-10);         assertEquals("chi-square p-value", 0.002512096, testStatistic.chiSquareTest(expected1, observed1), 1E-9); 16.413 is not correct because the expected values do not make sense, they should be: 521.19403 581.37313  88.11940  65.55224  39.76119 so that the sum of expected equals 1296 which is the sum of observed. Here is some R code (r-project.org) which proves it: > o1 [1] 500 623  72  70  31 > e1 [1] 485 541  82  61  37 > chisq.test(o1,p=e1,rescale.p=TRUE)         Chi-squared test for given probabilities data:  o1  X-squared = 9.0233, df = 4, p-value = 0.06052 > chisq.test(o1,p=e1,rescale.p=TRUE)$observed [1] 500 623  72  70  31 > chisq.test(o1,p=e1,rescale.p=TRUE)$expected [1] 521.19403 581.37313  88.11940  65.55224  39.76119 	https://issues.apache.org/jira/browse/MATH-175	src/java
103	src/java/org/apache/commons/math/distribution/NormalDistributionImpl.java内public void setStandardDeviation(double sd)	b8058233e92ad635fe204f450235cad597cd70f2	4ce05bcd51ec956d789d20b59c743603d24a8ab7	MATH-167	ConvergenceException in normal CDF	NormalDistributionImpl::cumulativeProbability(double x) throws ConvergenceException if x deviates too much from the mean. For example, when x=+/-100, mean=0, sd=1. Of course the value of the CDF is hard to evaluate in these cases, but effectively it should be either zero or one.	https://issues.apache.org/jira/browse/MATH-167	src/java
104	src/java/org/apache/commons/math/special/Gamma.java内noLeftCurtyIn1stLine:	c43fa5d5abc9a17a1bcc05831dfc42a5d0195bff	e39a7750ef362679599b623b05bfadb150832515	MATH-166	Special functions not very accurate	The Gamma and Beta functions return values in double precision but the default epsilon is set to 10e-9. I think that the default should be set to the highest possible accuracy, as this is what I'd expect to be returned by a double precision routine. Note that the erf function already uses a call to Gamma.regularizedGammaP with an epsilon of 1.0e-15.	https://issues.apache.org/jira/browse/MATH-166	src/java
105	src/java/org/apache/commons/math/stat/regression/SimpleRegression.java内public double getSlope()	ab1b9500fd4b6898757e9c74dc2eeae692b25146	fc21b26f84312e4f75e8b144238618c73a8b091f	MATH-85	[math]  SimpleRegression getSumSquaredErrors	getSumSquaredErrors returns -ve value. See test below: public void testSimpleRegression() {   double[] y =  {  8915.102, 8919.302, 8923.502} ;   double[] x =  { 1.107178495, 1.107264895, 1.107351295} ;   double[] x2 =  { 1.107178495E2, 1.107264895E2, 1.107351295E2} ;   SimpleRegression reg = new SimpleRegression();   for (int i = 0; i < x.length; i++)  {    reg.addData(x[i],y[i]);   }   assertTrue(reg.getSumSquaredErrors() >= 0.0); // OK   reg.clear();   for (int i = 0; i < x.length; i++)  {    reg.addData(x2[i],y[i]);   }   assertTrue(reg.getSumSquaredErrors() >= 0.0); // FAIL  }	https://issues.apache.org/jira/browse/MATH-85	src/java
106	src/java/org/apache/commons/math/fraction/ProperFractionFormat.java内public Fraction parse(String source, ParsePosition pos)外src/java/org/apache/commons/math/fraction/ProperFractionFormat.java内public Fraction parse(String source, ParsePosition pos)	5de83dca70afabf12bd46bb9ab438753c14c7453	41ba9e00e3bbde990f6821f67f0da2a5575b9ac3	MATH-60	[math] Function math.fraction.ProperFractionFormat.parse(String, ParsePosition) return illogical result	Hello, I find illogical returned result from function "Fraction parse(String source,  ParsePostion pos)" (in class ProperFractionFormat of the Fraction Package) of  the Commons Math library. Please see the following code segment for more  details: " ProperFractionFormat properFormat = new ProperFractionFormat(); result = null; String source = "1 -1 / 2"; ParsePosition pos = new ParsePosition(0); //Test 1 : fail  public void testParseNegative(){    String source = "-1 -2 / 3";    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source, pos);    assertNull(actual); } // Test2: success public void testParseNegative(){    String source = "-1 -2 / 3";    ParsePosition pos = new ParsePosition(0);    Fraction actual = properFormat.parse(source, pos);  // return Fraction 1/3    assertEquals(1, source.getNumerator());    assertEquals(3, source.getDenominator()); } " Note: Similarly, when I passed in the following inputs:    input 2: (source = “1 2 / -3”, pos = 0)   input 3: ( source = ” -1 -2 / 3”, pos = 0) Function "Fraction parse(String, ParsePosition)" returned Fraction 1/3 (means  the result Fraction had numerator = 1 and  denominator = 3)for all 3 inputs  above. I think the function does not handle parsing the numberator/ denominator  properly incase input string provide invalid numerator/denominator.  Thank you!	https://issues.apache.org/jira/browse/MATH-60	src/java